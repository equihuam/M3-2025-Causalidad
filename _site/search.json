[
  {
    "objectID": "presentaciones.html",
    "href": "presentaciones.html",
    "title": "M3-Presentaciones",
    "section": "",
    "text": "Leyes de la Ciencia\n\n\nDiseño de estudios y experimentos\n\n\n\npresentación\n\n\n\n\n\n\n\n\n\n29 ene 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nModelo Estadístico Lineal\n\n\nmodelación y prueba de hipótesis\n\n\n\npresentación\n\n\n\n\n\n\n\n\n\n29 ene 2024\n\n\nMiguel Equihua, Octavio Pérez Maqueo, Elio G. Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducción a Diagramas Causales\n\n\n\n\n\n\npresentación\n\n\n\n\n\n\n\n\n\n29 ene 2024\n\n\nMiguel Equihua, Octavio Pérez Maqueo, Elio G. Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nMedidas Repetidas\n\n\n\n\n\n\npresentación\n\n\n\n\n\n\n\n\n\n8 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nInfraestructura de Experimentación\n\n\nalgunos ejemplos\n\n\n\npresentación\n\n\n\n\n\n\n\n\n\n8 feb 2024\n\n\nMiguel Equihua, Octavio Pérez Maqueo, Elio G. Lagunes\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "ejercicios.html",
    "href": "ejercicios.html",
    "title": "M3-ejercicios",
    "section": "",
    "text": "Tarea de análisis críticos\n\n\n\n\n\n\ntaller\n\n\n\n\n\n\n\n\n\n1 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nTarea de análisis crítico - crudo\n\n\n\n\n\n\ntaller\n\n\n\n\n\n\n\n\n\n2 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nPizzas\n\n\n\n\n\n\ntaller\n\n\n\n\n\n\n\n\n\n2 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nEnsayo de antivenenos\n\n\n\n\n\n\ntaller\n\n\n\n\n\n\n\n\n\n2 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nvenenos\n\n\n\n\n\n\n\n\n\n\n\n6 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nMasa para Pizza\n\n\n\n\n\n\n\n\n\n\n\n7 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nControl de Tareas\n\n\n\n\n\n\nclase\n\n\n\n\n\n\n\n\n\n7 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nModelación con Stan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "ejercicios/stan-model/index.html",
    "href": "ejercicios/stan-model/index.html",
    "title": "Modelación con Stan",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLa modelación estadística tiene múltiples opciones actualmente. Señaladamente R, Python, Julia, Matlab, Stata y Stan que de acuerdo con su propia apreciación “es una plataforma de última generación para el modelado estadístico y el cálculo estadístico de alto rendimiento en la que confían miles de usuarios”. Stan es básicamente un lenguaje para la programación de modelos probabilísticos que permite:\nStan tiene su motor de cálculo propio, pero interactúa muy bien con los lenguajes de análisis de datos más populares (R, Python, MATLAB, etc.) y en los sistemas operativos comunes (Linux, Mac, Windows). Puedes obtener más información sobre esta propuesta de modelación estadística en la página de Stan."
  },
  {
    "objectID": "ejercicios/stan-model/index.html#preparación",
    "href": "ejercicios/stan-model/index.html#preparación",
    "title": "Modelación con Stan",
    "section": "Preparación",
    "text": "Preparación\nEl concepto que utiliza Stan parte de la especificación de las funciones de densidad que le interesan al usuario para enseguida ajustar los modelos a los datos. Un ejemplo trivial es el siguiente, en el que sólo nos proponemos estimar la media de una muestra de datos que asumiremos se distribuyen normalmente. Lo primero que hay que hacer es crear un archivo que especifique el modelo en los términos que Stan requiere. La especificación básica es la siguiente (se pueden agregar comentarios con un doble diagonal). Primero una definición del tipo de datos que requiere el modelo.\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nEn seguida se detallan los parámetros que definen el modelo.\n// The parameters accepted by the model. Our model\n// accepts two parameters 'mu' and 'sigma'.\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nFinalmente, se especifica el modelo. El modelo completo se guarda como un archivo stan al que se llamará cuando se busque realizar el ajuste.\n// The model to be estimated. We model the output\n// 'y' to be normally distributed with mean 'mu'\n// and standard deviation 'sigma'.\nmodel {\n  y ~ normal(mu, sigma);\n}\nPara ejemplificar el uso del modelo stan ya definido arriba, sólo nos resta preparar algunos datos de prueba, activar la biblioteca rstan y realizar el ajuste, que básicamente consiste en generar un muestreo de la distribución de probabilidades. Al invocar la función stan se compila la especificación del modelo para que Stan lo pueda procesar. Una vez hecho eso, se realiza un proceso de muestreo de las distribución conjunta especificada, lo qe equivale a ajustar el modelo en Stan.\n\nlibrary(rstan)\n\nCargando paquete requerido: StanHeaders\n\n\n\nrstan version 2.32.6 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\n\nDo not specify '-march=native' in 'LOCAL_CPPFLAGS' or a Makevars file\n\n\n\nAdjuntando el paquete: 'rstan'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(tibble)\nrstan_options(auto_write = TRUE)\n\ndatos &lt;-  list(N = 1000, \n               y = rnorm(1000, 10, 2))\n               \nnorm_fit &lt;- stan(file = 'modelo.stan', data = datos)\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.45 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.046 seconds (Warm-up)\nChain 1:                0.045 seconds (Sampling)\nChain 1:                0.091 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.043 seconds (Warm-up)\nChain 2:                0.054 seconds (Sampling)\nChain 2:                0.097 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.52 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.051 seconds (Warm-up)\nChain 3:                0.042 seconds (Sampling)\nChain 3:                0.093 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.3e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.069 seconds (Warm-up)\nChain 4:                0.057 seconds (Sampling)\nChain 4:                0.126 seconds (Total)\nChain 4: \n\n\nUn objeto stanfit en R contiene los resultados derivados de ajustar (muestrear) un modelo Stan utilizando métodos Monte Carlo para cadenas de Markov (es la que se usa por default) o alguna de las aproximaciones variacionales que es capaz de procesar Stan.\nAhora podemos ver los resultados del ajuste del modelo y explorar los resultados que arroja.\n\nsummary(norm_fit)\n\n$summary\n              mean      se_mean         sd         2.5%          25%\nmu        9.985783 0.0011355597 0.06614853     9.855086     9.941181\nsigma     2.026648 0.0007744342 0.04421861     1.945260     1.996270\nlp__  -1205.628069 0.0248991890 1.03721594 -1208.246434 -1206.024870\n               50%          75%       97.5%    n_eff      Rhat\nmu        9.986748    10.030374    10.11353 3393.285 0.9998499\nsigma     2.025854     2.055898     2.11594 3260.177 1.0000536\nlp__  -1205.314867 -1204.911103 -1204.64576 1735.274 1.0005728\n\n$c_summary\n, , chains = chain:1\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu        9.985182 0.06568395     9.856185     9.940864     9.984545\n    sigma     2.025361 0.04386068     1.947783     1.993357     2.024739\n    lp__  -1205.614593 0.96288322 -1208.149782 -1206.029611 -1205.315540\n         stats\nparameter          75%        97.5%\n    mu       10.032699    10.110996\n    sigma     2.053814     2.110686\n    lp__  -1204.914020 -1204.641254\n\n, , chains = chain:2\n\n         stats\nparameter        mean         sd         2.5%          25%          50%\n    mu        9.98513 0.06777224     9.845464     9.941350     9.986006\n    sigma     2.02700 0.04344609     1.946390     1.996806     2.025546\n    lp__  -1205.63887 1.17127274 -1208.395582 -1205.974116 -1205.285256\n         stats\nparameter          75%        97.5%\n    mu       10.028296    10.118765\n    sigma     2.055042     2.114998\n    lp__  -1204.892783 -1204.646001\n\n, , chains = chain:3\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu        9.986934 0.06591295     9.862552     9.941039     9.987669\n    sigma     2.028308 0.04538603     1.945901     1.998069     2.028165\n    lp__  -1205.646790 0.99467206 -1208.199104 -1206.054733 -1205.359767\n         stats\nparameter          75%        97.5%\n    mu       10.032883    10.110454\n    sigma     2.057687     2.122019\n    lp__  -1204.938112 -1204.657286\n\n, , chains = chain:4\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu        9.985884 0.06528054     9.856620     9.941761     9.987575\n    sigma     2.025920 0.04416708     1.945046     1.996571     2.025995\n    lp__  -1205.612018 1.00856061 -1208.428302 -1206.005838 -1205.309293\n         stats\nparameter          75%        97.5%\n    mu       10.027844    10.115235\n    sigma     2.056599     2.115486\n    lp__  -1204.895888 -1204.644620\n\najuste &lt;- as_tibble(rstan::extract(norm_fit))\n\nhead(ajuste)\n\n# A tibble: 6 × 3\n         mu     sigma      lp__\n  &lt;dbl[1d]&gt; &lt;dbl[1d]&gt; &lt;dbl[1d]&gt;\n1      9.93      2.02    -1205.\n2      9.98      2.07    -1205.\n3     10.1       1.96    -1209.\n4     10.1       2.06    -1206.\n5      9.96      2.07    -1205.\n6     10.0       1.93    -1207.\n\nhead(as_tibble(as.data.frame(norm_fit)))\n\n# A tibble: 6 × 3\n     mu sigma   lp__\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1  9.97  2.04 -1205.\n2  9.92  2.00 -1205.\n3  9.92  2.00 -1205.\n4 10.1   2.06 -1206.\n5 10.1   2.03 -1205.\n6  9.97  2.03 -1205.\n\nmean(ajuste$mu)\n\n[1] 9.985783"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "M3 - Diseño de Experimentos",
    "section": "",
    "text": "Bienvenidos al Módulo 3 de estadística\n\n\n\n\n\n\noperación\n\n\n\n\n\n\n\n\n\n27 ene 2025\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nConceptos y Modelos en la experimentación\n\n\n\n\n\n\nclase\n\n\n\n\n\n\n\n\n\n27 ene 2025\n\n\nMiguel Equihua, Octavio Pérez Maqueo, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nModelos de medidas repetidas\n\n\n\n\n\n\nclase\n\n\n\n\n\n\n\n\n\n8 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nExperimentación Ecológica\n\n\n\n\n\n\ntaller\n\n\n\n\n\n\n\n\n\n8 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nModelos de Efectos Mixtos\n\n\n\n\n\n\nclase\n\n\n\n\n\n\n\n\n\n7 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nModelos anidados\n\n\n\n\n\n\nclase\n\n\n\n\n\n\n\n\n\n6 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nRestricciones a la aleatorización\n\n\n\n\n\n\nclase\n\n\n\n\n\n\n\n\n\n2 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nExperimentos completamente aleatorizados\n\n\n\n\n\n\nclase\n\n\n\n\n\n\n\n\n\n1 feb 2024\n\n\nMiguel Equihua, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nDiagramas Causales\n\n\n\n\n\n\nclase\n\n\n\n\n\n\n\n\n\n31 ene 2024\n\n\nMiguel Equihua, Octavio Pérez Maqueo, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nLeyes de la Ciencia\n\n\n\n\n\n\nclase\n\n\n\n\n\n\n\n\n\n29 ene 2024\n\n\nMiguel Equihua, Octavio Pérez Maqueo, Elio Lagunes\n\n\n\n\n\n\n\n\n\n\n\n\nEl Modelo estadístico lineal\n\n\n\n\n\n\nclase\n\n\n\n\n\n\n\n\n\n29 ene 2024\n\n\nMiguel Equihua, Octavio Pérez Maqueo, Elio Lagunes\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#instrucciones-para-participar",
    "href": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#instrucciones-para-participar",
    "title": "Leyes de la Ciencia",
    "section": "Instrucciones para participar",
    "text": "Instrucciones para participar\n\n\n\n\n\n\nlo que hay que hacer es\n\n\n\nEn tu computadora ir vevox.app\nPara usar el celular instala Vevox desde la tienda de apps"
  },
  {
    "objectID": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#para-ti-qué-es-la-ciencia",
    "href": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#para-ti-qué-es-la-ciencia",
    "title": "Leyes de la Ciencia",
    "section": "1. Para ti, ¿qué es la ciencia?",
    "text": "1. Para ti, ¿qué es la ciencia?\n\nParticipa: vevox.app ID: 152-551-925\n\n\n\n\n\n\n\n \n\n\n\n\n(escribe de 3 a 20 palabras que reflejan tu percepción)\n\n\n\n\nEscribe una palabra y oprime enviar"
  },
  {
    "objectID": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#supuestos-que-se-hacen-al-hacer-ciencia",
    "href": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#supuestos-que-se-hacen-al-hacer-ciencia",
    "title": "Leyes de la Ciencia",
    "section": "Supuestos que se hacen al hacer ciencia",
    "text": "Supuestos que se hacen al hacer ciencia\n\nToda argumentación racional inicia con ciertos supuestos .\nEstos supuestos típicamente quedan implícitos en la práctica de las actividades científicas\nConviene hacerlas explícitas para darle claridad a nuestra forma de hacer ciencia."
  },
  {
    "objectID": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#qué-damos-por-sentados-cuando-hacemos-ciencia",
    "href": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#qué-damos-por-sentados-cuando-hacemos-ciencia",
    "title": "Leyes de la Ciencia",
    "section": "2. ¿Qué damos por sentados cuando hacemos ciencia?",
    "text": "2. ¿Qué damos por sentados cuando hacemos ciencia?\n\nParticipa: vevox.app ID: 152-551-925\n\n\n\n\n\n\n\n \n\n\n\n\n(escribe de 3 a 20 palabras que reflejan tu percepción)\n\n\n\n\nEscribe una palabra y oprime enviar"
  },
  {
    "objectID": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#algunos-otros-corolarios-interesantes",
    "href": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#algunos-otros-corolarios-interesantes",
    "title": "Leyes de la Ciencia",
    "section": "Algunos otros corolarios interesantes",
    "text": "Algunos otros corolarios interesantes\n\nLa naturaleza es comprensible . Einstein afirmaba que: la cosa más incomprensible del universo es que sea comprensible.\nLa naturaleza es uniforme . Es decir los procesos y patrones observados sólo en una escala limitada se mantendrán universalmente (esto es obviamente imprescindible en ciencias como la Astronomía).\n\n¿Qué pasa con este supuesto en ciencias como la Ecología, la Psicología o las Ciencias sociales? De paso, hay que notar que este supuesto implica la homogeneidad del material experimental.\n\nLa causalidad existe . El principio de causalidad es la noción que consiste en que “cada evento (o fenómeno) natural se supone tiene una causa, de modo que si tal situación causal puede ser restituida, el evento será duplicado” (Underwood1957)."
  },
  {
    "objectID": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#cómo-debe-interpretarse-esto",
    "href": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#cómo-debe-interpretarse-esto",
    "title": "Leyes de la Ciencia",
    "section": "3. ¿Cómo debe interpretarse esto?",
    "text": "3. ¿Cómo debe interpretarse esto?\n\nParticipa: vevox.app ID: 152-551-925\n\n\n\n\n\n\n\n \n\n\n\n\n\n90% de las semillas en un grupo tratado germinan.\n20% lo hacen en el grupo control.\n\n\n\n\n\nElije la respuesta que consideres más apropiada"
  },
  {
    "objectID": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#qué-tan-de-acuerdo-estás-con-la-siguiente-aseveración",
    "href": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#qué-tan-de-acuerdo-estás-con-la-siguiente-aseveración",
    "title": "Leyes de la Ciencia",
    "section": "4. ¿Qué tan de acuerdo estás con la siguiente aseveración?",
    "text": "4. ¿Qué tan de acuerdo estás con la siguiente aseveración?\n\nParticipa: vevox.app ID: 152-551-925\n\n“La causalidad implica correlación”"
  },
  {
    "objectID": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#causalidad-finita-y-desprecio",
    "href": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#causalidad-finita-y-desprecio",
    "title": "Leyes de la Ciencia",
    "section": "Causalidad finita y desprecio…",
    "text": "Causalidad finita y desprecio…\n\nPara Bachelard, uno de los signos distintivos del espíritu científico y del espíritu filosófico es el derecho a despreciar . Para él, el espíritu científico explicita clara y distintamente este derecho a despreciar lo despreciable\nDerecho que incansablemente el espíritu filosófico le rehúsa.\nPara ilustrar esto, recurre a Ostwald:\n\n“Cualquiera que sea el fenómeno considerado, siempre hay un número extremadamente grande de circunstancias que no tienen influencia mesurable sobre él”.\n¿Cómo influye el color de un proyectil en sus propiedades balísticas?"
  },
  {
    "objectID": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#conoces-alguna-metodología-científica-para-describir-y-analizar-estructuras-de-relación-causa-efecto",
    "href": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#conoces-alguna-metodología-científica-para-describir-y-analizar-estructuras-de-relación-causa-efecto",
    "title": "Leyes de la Ciencia",
    "section": "5. ¿Conoces alguna metodología científica para describir y analizar estructuras de relación causa efecto?",
    "text": "5. ¿Conoces alguna metodología científica para describir y analizar estructuras de relación causa efecto?\n\nParticipa: vevox.app ID: 152-551-925"
  },
  {
    "objectID": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#cuáles-son-los-aspectos-clave-para-diseñar-un-experimentoestudio",
    "href": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#cuáles-son-los-aspectos-clave-para-diseñar-un-experimentoestudio",
    "title": "Leyes de la Ciencia",
    "section": "6 ¿Cuáles son los aspectos CLAVE para diseñar un experimento/estudio?",
    "text": "6 ¿Cuáles son los aspectos CLAVE para diseñar un experimento/estudio?\n\nParticipa: vevox.app ID: 152-551-925"
  },
  {
    "objectID": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#section-5",
    "href": "presentaciones/02-leyes-de-la-ciencia/pres-leyes-ciencia.html#section-5",
    "title": "Leyes de la Ciencia",
    "section": "",
    "text": "M3-Causalidad"
  },
  {
    "objectID": "presentaciones/03-modelo-lineal/pres-modelo-lineal.html#el-modelo-estadístico-lineal-generalcenter",
    "href": "presentaciones/03-modelo-lineal/pres-modelo-lineal.html#el-modelo-estadístico-lineal-generalcenter",
    "title": "Modelo Estadístico Lineal",
    "section": "El modelo estadístico lineal general{center}",
    "text": "El modelo estadístico lineal general{center}\nEstructura general"
  },
  {
    "objectID": "presentaciones/03-modelo-lineal/pres-modelo-lineal.html#ejemplo-de-modelo-lineal",
    "href": "presentaciones/03-modelo-lineal/pres-modelo-lineal.html#ejemplo-de-modelo-lineal",
    "title": "Modelo Estadístico Lineal",
    "section": "Ejemplo de modelo lineal",
    "text": "Ejemplo de modelo lineal\n\nGanancia de peso en un grupo de orugas que declina conforme se incrementa el contenido de taninos en la dieta.\nEsta condición puede describirse en forma abreviada así:\n\nganancia de peso de cada oruga=ganancia de peso base en general+efecto del contenido de taninos en la dieta+efecto de otros factores que fluctuan aleatoriamente\nComo modelo lineal se puede escribir así :\n\\[\ny_{ij} = \\beta_0 x_0j + \\sum_{i=1}^{k}\\beta_i x_{ij} + \\varepsilon_{j(i)}  \n\\]"
  },
  {
    "objectID": "presentaciones/03-modelo-lineal/pres-modelo-lineal.html#modelos-con-variables-explicativas-cualitativas",
    "href": "presentaciones/03-modelo-lineal/pres-modelo-lineal.html#modelos-con-variables-explicativas-cualitativas",
    "title": "Modelo Estadístico Lineal",
    "section": "Modelos con variables explicativas cualitativas",
    "text": "Modelos con variables explicativas cualitativas\n\n\n\n\n\n\nUn agrónomo planea estudiar las tasas de producción de cuatro híbridos de trigo en tres regiones geográficas representantes de diferentes condiciones de sequía. Los sitios se escogen según la cantidad de lluvia como normal climatológica y la respuesta es el rendimiento por hectárea. Las semillas de los híbridos son asignadas aleatoriamente a los sitios.\n\n\n¿cómo piensas que se podría hacer esta aleatorización.\n\n\n\n\n¿cual sería el modelo lineal que lo podría describir?\n\n\n\n\\[\ny_{ijk} = \\mu + R_i + H_j + RH_{ij} +  \\varepsilon_{k(ij)}\n\\]"
  },
  {
    "objectID": "presentaciones/03-modelo-lineal/pres-modelo-lineal.html#el-modelo-en-todo-su-esplendor",
    "href": "presentaciones/03-modelo-lineal/pres-modelo-lineal.html#el-modelo-en-todo-su-esplendor",
    "title": "Modelo Estadístico Lineal",
    "section": "El modelo en todo su esplendor",
    "text": "El modelo en todo su esplendor\n\\[\n\\begin{gather*}\ny_{ijk} = \\mu + \\beta_{1k}R_{1k} + \\beta_{2k}R_{2k} + \\beta_{3k}R_{3k} + \\\\\n          \\beta_{4k}H_{1k}   + \\beta_{5k}H_{2k}   + \\beta_{6k}H_{3k}   + \\beta_{7k}H_{ik} + \\\\\n          \\beta_{8k}RH_{1k}  + \\beta_{9k}RH_{2k}  + \\beta_{10k}RH_{3k} + \\beta_{11k}RH_{ik}+ \\\\\n          \\beta_{12k}RH_{1k} + \\beta_{13k}RH_{2k} + \\beta_{14k}RH_{3k} + \\beta_{15k}RH_{ik}+\\\\\n          \\beta_{16k}RH_{1k} + \\beta_{17k}RH_{2k} + \\beta_{18k}RH_{3k} + \\beta_{19k}RH_{ik}+\n          \\varepsilon_{k(ij)}\n\\end{gather*}\n\\]"
  },
  {
    "objectID": "presentaciones/03-modelo-lineal/pres-modelo-lineal.html#qué-valores-toman-las-x-las-r-y-las-h",
    "href": "presentaciones/03-modelo-lineal/pres-modelo-lineal.html#qué-valores-toman-las-x-las-r-y-las-h",
    "title": "Modelo Estadístico Lineal",
    "section": "¿Qué valores toman las X, las R y las H?",
    "text": "¿Qué valores toman las X, las R y las H?\nLa forma más común de modelar datos cualitativos es:\n\n\n\n\nMediante variables usualmente llamadas factores.\nUn factor es una listas de nombres o códigos de identificación de estados o niveles mutuamente excluyentes.\nEn la modelación se requiere convertir el factor a variables indicadoras o dummy.\nHabrá tantas variables dummy como estados o niveles tenga el factor\nCada variable se construye anotando la presencia/ausencia de la condición del factor.\n\n\n\n\n\\[\nH_1 = \\left\\{\n         \\begin{align*}\n           \\text{si } \\color{red}{sí} \\text{ es híbrido del tipo } a &: 1 \\\\\n           \\text{si } \\color{red}{no} \\text{ es híbrido del tipo } a &: 0\n         \\end{align*}\n      \\right\\}\n\\]"
  },
  {
    "objectID": "presentaciones/09-medidas-repetidas/medidas-repetidas.html#casos-familiares-en-ecología",
    "href": "presentaciones/09-medidas-repetidas/medidas-repetidas.html#casos-familiares-en-ecología",
    "title": "Medidas Repetidas",
    "section": "Casos familiares en ecología",
    "text": "Casos familiares en ecología\n\\[  \n\\begin{align*}\nn_{t+1} &= rn_t \\\\\nn_{t+1} &= rn_t (1 - \\alpha n_t) \\\\\n\\end{align*}\n\\]\n\nEsta última versión de crecimiento logístico se ve así arreglando un poco los términos \\[  \nn_{t-1} = rn_t - r \\alpha n_t^2\n\\]\nOtra forma de incorporar densodependencia\n\n\\[\nn_{t+1} = rn_t e^{-an_t}\n\\]\n\n¿se puede arreglar ésta de alguna forma conveniente?"
  },
  {
    "objectID": "presentaciones/09-medidas-repetidas/medidas-repetidas.html#ventajas-de-los-diseños-con-medidas-repetidas",
    "href": "presentaciones/09-medidas-repetidas/medidas-repetidas.html#ventajas-de-los-diseños-con-medidas-repetidas",
    "title": "Medidas Repetidas",
    "section": "Ventajas de los diseños con medidas repetidas",
    "text": "Ventajas de los diseños con medidas repetidas\n\nSe obtiene más información de cada sujeto comparada con la que normalmente se obtiene en los diseños convencionales.\nEl número de sujetos necesarios para el estudio, dentro de un nivel dado de potencia estadística, tiende a ser mucho menor.\nComo las observaciones son hechas dentro de los sujetos, la variabilidad debida a las diferencias individuales entre los sujetos se elimina del término de error.\nCada sujeto actúa como su propio control, incrementando la potencia estadística del estudio."
  },
  {
    "objectID": "presentaciones/11-experimentar/experimentar.html#free-air-carbon-dioxide-enrichment",
    "href": "presentaciones/11-experimentar/experimentar.html#free-air-carbon-dioxide-enrichment",
    "title": "Infraestructura de Experimentación",
    "section": "Free-Air Carbon Dioxide Enrichment",
    "text": "Free-Air Carbon Dioxide Enrichment\n\nEs un estudio multidisciplinar para evaluar los efectos del aumento de los niveles de ozono troposférico y dióxido de carbono en la estructura y función de los ecosistemas forestales del norte.\nDispone de doce anillos de 30 m en los que se pueden controlar las concentraciones de dióxido de carbono y ozono troposférico.\nEl diseño permite evaluar los efectos de estos gases solos y combinados en muchos atributos del ecosistema, como el crecimiento, el desarrollo de las hojas, las características de las raíces y el carbono del suelo. Al no haber confinamiento, no se produce ningún cambio significativo en el entorno natural, aparte de la elevación de las concentraciones de los gases traza añadidos.\nAspenFACE Home Page (mtu.edu)"
  },
  {
    "objectID": "presentaciones/11-experimentar/experimentar.html#instituto-bermuda-de-ciencias-oceánicas",
    "href": "presentaciones/11-experimentar/experimentar.html#instituto-bermuda-de-ciencias-oceánicas",
    "title": "Infraestructura de Experimentación",
    "section": "Instituto Bermuda de Ciencias Oceánicas",
    "text": "Instituto Bermuda de Ciencias Oceánicas\n\nSus experimentos se preguntan:\n\n¿Desarrollará la generación actual de corales mayor capacidad para hacer frente al estrés en respuesta al que experimenta?\n¿Se adaptará mejor la próxima generación de corales a las condiciones ambientales experimentadas por sus progenitores?.\n\n\nThe Next Generation of Coral: What Can It Teach Us? | Currents | BIOS - Bermuda Institute of Ocean Sciences"
  },
  {
    "objectID": "presentaciones/11-experimentar/experimentar.html#centro-de-recursos-naturales",
    "href": "presentaciones/11-experimentar/experimentar.html#centro-de-recursos-naturales",
    "title": "Infraestructura de Experimentación",
    "section": "Centro de Recursos Naturales",
    "text": "Centro de Recursos Naturales\nMediante la recopilación y el examen de pruebas de la eficacia de distintos enfoques de gestión forestal, los responsables de la toma de decisiones, los científicos y las partes interesadas pueden conocer las opciones que determinarán la sostenibilidad de estos ecosistemas críticos.\nEl objetivo del estudio es examinar cómo los ecosistemas sostenibles pueden tener en cuenta tanto el bienestar del entorno forestal como el de las comunidades humanas\nT3 Watershed Experiment | Olympic Natural Resources Center (washington.edu)"
  },
  {
    "objectID": "presentaciones/11-experimentar/experimentar.html#qué-alternativas-hay-a-esto",
    "href": "presentaciones/11-experimentar/experimentar.html#qué-alternativas-hay-a-esto",
    "title": "Infraestructura de Experimentación",
    "section": "¿Qué alternativas hay a esto?",
    "text": "¿Qué alternativas hay a esto?"
  },
  {
    "objectID": "presentaciones/11-experimentar/experimentar.html#section-9",
    "href": "presentaciones/11-experimentar/experimentar.html#section-9",
    "title": "Infraestructura de Experimentación",
    "section": "",
    "text": "M3-Causalidad"
  },
  {
    "objectID": "posts/00-Bienvenida/index.html#objetivos-del-módulo-3",
    "href": "posts/00-Bienvenida/index.html#objetivos-del-módulo-3",
    "title": "Bienvenidos al Módulo 3 de estadística",
    "section": "Objetivos del Módulo 3",
    "text": "Objetivos del Módulo 3\nDurante las últimas semanas revisaron conceptos de probabilidad y matemáticas que necesitamos como un lenguaje eficiente de comunicación. También empezaron a explorar como es que se pueden analizar proposiciones sobre la existencia de asociación o incluso relaciones de dependencia entre dos variables: modelos de regresión simple. Ahora vamos a aplicar y extender estos aprendizajes para abordar el desafío de producir conocimiento que nos permita comprender como funciona el mundo. Asumir un interés en la comprensión causal requiere desarrollar las habilidades de pensamiento crítico, lo que constituye por tanto otro de los propósitos del módulo.\nUtilizaremos el lenguaje de programación R como plataforma de cómputo para el análisis de datos. Aspiramos a ofrecerles así un curso introductorio para su uso. También nos interesa acercarnos a los enfoques formales para el análisis causal actual, mediante la formulación de Grafos Acíclicos Dirigidos (DAG). Los invitamos a hacer explícitas y a dibujar las relaciones causales de sus proyectos para comprenderlas, comunicarlas y analizarlas con mayor eficiencia.\nComo ejercicio inicial les pedimos preparen y nos entreguen en una sola cuartilla la descripción de una de las preguntas de investigación que se han planteado en su proyecto de posgrado, con suficiente detalle como para comprender el asunto y la propuesta sobre como poner a prueba la idea planteada. No se trata de todo el protocolo de su proyecto de investigación. Escojan sólo un aspecto de él que deseen compartir y explorar en este curso como oportunidad de aprendizaje."
  },
  {
    "objectID": "posts/00-Bienvenida/index.html#plan-de-clase",
    "href": "posts/00-Bienvenida/index.html#plan-de-clase",
    "title": "Bienvenidos al Módulo 3 de estadística",
    "section": "Plan de clase",
    "text": "Plan de clase\n\n\nCódigo\ndias &lt;- rep(NA, 31+28) \ndias[c(27:28, 30:31, 34:35, 37)] &lt;- \"M3 clase\"\ndias[c(29, 34, 37)] &lt;- \"M3 lectura\"\ndias[38] &lt;- \"M3 Mini simposio\"\ndias[c(25:26, 32,33, 36, 39:40)] &lt;- \"Descanso\"\n\na &lt;- calendR(from = \"2025-01-01\",\n        to = \"2025-2-28\",\n        special.days = dias,\n        special.col = c(\"gray\", \"lightgreen\", \"pink\", \"gold\"),\n        legend.pos = \"right\")\n\ndias &lt;- rep(NA, 31) \ndias[c(27:28, 30:31)] &lt;- \"M3 clase\"\ndias[c(29)] &lt;- \"M3 lectura\"\ndias[c(25:26)] &lt;- \"Descanso\"\n\ncalendR(month = 1,\n        lunar = TRUE,\n        special.days = dias,\n        special.col = c(\"gray\", \"lightgreen\", \"pink\"),\n        legend.pos = \"right\")\n\n\n\n\n\n\n\n\n\nCódigo\ndias &lt;- rep(NA, 28) \ndias[c(3:4, 6)] &lt;- \"M3 clase\"\ndias[c(5)] &lt;- \"M3 lectura\"\ndias[7] &lt;- \"M3 Mini simposio\"\ndias[c(1,2, 5, 8:10)] &lt;- \"Descanso\"\n\ncalendR(month = 2,\n        lunar = TRUE,\n        special.days = dias,\n        special.col = c(\"gray\", \"lightgreen\", \"pink\"))\n\n\n\n\n\n\n\n\n\nCódigo\ncontenido &lt;- read_csv(\"contenido.csv\", col_names = TRUE, show_col_types = FALSE)\ncontenido$lectura &lt;- c(\" \", \"debate\")[contenido$lectura + 1]\n\ncontenido %&gt;% flextable(col_keys = c(\"dia\", \"lectura\", \"tema\")) %&gt;% \n  set_header_labels(values = list(dia = \"Día\", \n                                  lectura= \"Lectura\", \n                                  tema = \"Tema\")) %&gt;% \n  autofit() %&gt;% \n  theme_zebra(odd_body = c(\"lightcyan1\")) %&gt;% \n  bold(i = 1:9, j = 1)\n\n\nCalendario de actividades del Módulo 3DíaLecturaTemaLunes 27 Un lenguaje para describir modelos (diseño de experimentos y de tratamientos)Martes 28 Causalidad y modelación gráfica: Grafos Acíclicos Dirigidos (DAG)Miércoles 29debateDiseño de experimentos: hipótesis y la modelación estadísticaJueves 30 Efectos fijos y aleatorios sus implicaciones en los modelos estadísticosViernes 31 Evitar confusión por variables conocidas: Bloques aleatorizadosLunes 3debateMuestreo dentro unidades experimentales: Diseños anidadosMartes 4 Unidades experimentales múltinivel y anidadas: Diseños de Parcelas divididasJueves 6debateModelos jerárquicos (incluye medidas repetidas)Viernes 7 Feria del diseño para discutir los protocolos de los estudiantes"
  },
  {
    "objectID": "posts/00-Bienvenida/index.html#lecturas",
    "href": "posts/00-Bienvenida/index.html#lecturas",
    "title": "Bienvenidos al Módulo 3 de estadística",
    "section": "Lecturas",
    "text": "Lecturas\nComo parte de las experiencias de aprendizaje que realizaremos tenemos tres lecturas. Les pedimos que hagan una lectura crítica de los textos propuestos y que preparen un comentario que contenga sus apreciaciones en favor o en contra de los argumentos presentados. También esperamos comenten sobre las implicaciones amplias y para la práctica científica en lo general, de los argumentos que exponen los autores. Sus reacciones tendrán oportunidad de ser expuestas y debatidas en un espacio del programa del módulo 3 a lo largo de 60 minutos.\nlas lecturas son:\n\nbelief in conspiracy theories\nguide to model selection and causal inference\nSame data, different analysts"
  },
  {
    "objectID": "posts/00-Bienvenida/index.html#evaluación",
    "href": "posts/00-Bienvenida/index.html#evaluación",
    "title": "Bienvenidos al Módulo 3 de estadística",
    "section": "Evaluación",
    "text": "Evaluación\n\nEl Módulo III será evaluado a través de las tareas y controles de lectura que les pediremos que hagan conforme se desarrolle el curso. La participación en quizes y preguntas en línea será considerada como una oportunidad de mejora, aunque la no participación no se considerará para disminuir la calificación."
  },
  {
    "objectID": "posts/00-Bienvenida/index.html#estudiantes",
    "href": "posts/00-Bienvenida/index.html#estudiantes",
    "title": "Bienvenidos al Módulo 3 de estadística",
    "section": "Estudiantes",
    "text": "Estudiantes\n\n\nCódigo\nestudiantes_2024 &lt;- read_excel(\"Lista-de-alumnos-2025.xlsx\", skip = 0)\n\nestudiantes_2024 %&gt;% \n  arrange(Estudiante) %&gt;% \n  flextable(col_keys = c(\"Matrícula\", \"Estudiante\")) %&gt;%\n  bg(i = ~str_detect(Adscripción, \"baja\"), bg = \"gray80\") %&gt;% \n  autofit() %&gt;%\n  theme_vanilla()\n\n\nEstudianteCeballos Vargas Heidi DanielaChávez Sánchez Jesús OmarContreras Martínez GuadalupeElizondo Salas Andrea CarolinaFlores Zarza BrayanHernández Navarro Héctor SantiagoJiménez Acosta NinfaLeon Wilchez Yeli YesseniaMartínez Sáenz Juan PabloMorales Cid Luis RobertoMÁRQUEZ GUERRA SONIAOlmedo Blanco María del CarmenPavón Acosta DenissePizarro Ortiz Sergio CristobalPérez Morales Juan ManuelRamírez Solano María AngélicaReyes Jiménez Ottmar RaymundoRodríguez Becerra Sared HelenaTorralva Apodaca ErickValdez Ojeda Luis FernandoVargas Amaya Andrés Felipe"
  },
  {
    "objectID": "posts/01-concepto-y-modelos-experimentar/index.html",
    "href": "posts/01-concepto-y-modelos-experimentar/index.html",
    "title": "Conceptos y Modelos en la experimentación",
    "section": "",
    "text": "McElreath (2020) presenta en su libro el hipotético ejemplo de la prueba de sangre para vampirismo. Propone que hay un análisis de sangre que detecta correctamente 95% de las veces, la afiliación de un individuo al linaje del conde Drácula y los inmortales vampiros. En notación matemática:\n\\[\n{Pr(resultado~ positivo~de~la~prueba|vampiro) = 0.95}\n\\]\nEs una prueba muy precisa, casi siempre identificando vampiros reales. Sin embargo, también comete errores y produce falsos positivos. Es así que el uno por ciento de las veces diagnostica incorrectamente a los simples mortales como vampiros:\n\\[\n{Pr(resultado~positivo~de~la~prueba|mortal) = 0.01}\n\\]\nLa última pieza de información que necesitamos es saber que los vampiros en realidad son bastante raros. Sólo el 0.1% de la población lo es, lo que implica:\n\\[\n{Pr(vampiro) = 0.001}\n\\]\nA partir de este conocimiento científico, supongamos que un amigo da positivo en el test de vampirismo.\n\n\n\n\n\n\n¿Cuál es la probabilidad de que sea un inmortal chupasangre\n\n\n\n\n\nEl enfoque de investigación formal empezaría por usar el teorema de Bayes para deducir la probabilidad \\({Pr(vampiro|positivo)}\\), lo que en cierta forma implica “invertir la probabilidad”, pues lo que ahora sabemos es el valor de \\({Pr(positivo|vampiro)}\\). El cálculo puede presentarse como:\n\\[\n\\Pr(vampiro|positivo) = \\frac{Pr(positivo|vampiro) \\times Pr(vampiro)}{Pr(positivo)}\n\\]\nen donde \\({Pr(positivo)}\\) es la probabilidad promedio de los resultados positivos de la prueba, es decir,\n\\[\nPr(positivo) = Pr(positivo|vampiro)\\times Pr(vampiro) + Pr(positivo|mortal) \\times ({1 − Pr(vampiro)})\n\\]\n\n\n\nTodo esto lo podemos hacer en R.\nPrimero tomamos nota de lo que ya sabemos a partir del enunciado anterior, ¡a priori!\n\n\n\n\n\n\n\n\nSi queremos verificar los datos podemos imprimirlos con la función cat:\n\n\n\n\n\n\n\n\nTomamos la fórmula de Bayes para invertir la probabilidad, pues queremos saber qué está pasando cuando tenemos la fortuna de toparnos con un resultado positivo en la prueba de sangre:\n\\[\nPr(positivo|vampiro)\n\\]\nEsto equivale a preguntarnos, dado que ya vimos el resultado científico que significa la prueba de sangre, ¿será vampiro el sujeto de quien se obtuvo esa muestra?:\n\\[\nPr(vampiro|positivo)\n\\]\n\n\n\n\n\n\n\n\nPor lo tanto, esta es la probabilidad de que el amigo sea en realidad un vampiro.\n\n\n\n\n\n\n¿Encuentras este resultado afin o contrario a lo que pensabas antes de hacer los cálculos?\n\n\n\n\n\n\nEste es un resultado muy importante. Exactamente así, o algo muy parecido, es el procedimiento que se sigue en muchos contextos de prueba realistas: las pruebas de PCR, antígeno o anticuerpos para SarsCov-2, la prueba del VIH la del DNA en un perfil criminal y por supuesto la prueba de significancia estadística.\nQuizás ayude a mejorar la intuición que tenemos de las cosas el considerar que siempre que la condición de interés sea muy rara, desarrollar una prueba excelente, capaz de diagnosticar bien todos los casos verdaderos (aunque invitablemente produzca también algunos falsos positivos), no es garantía suficiente de que un resultado positivo en general conlleve mucha información.\nLa razón es que usualmente resulta inevitable tener falsos positivos y por simple aritmética, esos casos serán la mayoría de los resultados que tendremos, incluso si todos los verdaderos positivos fueran detectados correctamente.\nAunque, como dice McElreath, no hay nada particularmente bayesiano aqui. Podríamos pensar que la ecuación que usamos aquí salio de la nada, aunque quizás la recuerdes de algún curso previo, de alguna charla interesante por ahí o incluso de lo que viste con Rosario ¡hace unas semanas!\nQuizás el ejemplo puede verse en forma más intuitiva utilizando otra narrativa para comprender lo que está ocurriendo. Digamos que en lugar de informar sobre las probabilidades, como antes, te digo lo siguiente:\n\nEn una población de 100,000 personas, 100 de ellas son vampiros.\nDe los 100 que son vampiros 95 darán positivo en la prueba de vampirismo.\nDe los 99,900 simples mortales, 999 darán positivo a la prueba de vampirismo.\n\nAhora piensa en esto, si hacemos pruebas a las 100,000 personas, ¿qué proporción de los que dan positivo en las pruebas de vampirismo son realmente vampiros?\nMuchas personas, aunque ciertamente no todas, encuentran esta forma de contar la historia mucho más fácil. Sigamos por este camino.\n\n\n\n\n\n\n¿Qué tal si contamos el número de personas que dan positivo?\n\n\n\n\n\n\\[\n95 + 999 = 1094\n\\] De estas 1094 pruebas positivas, 95 de ellas son vampiros reales, lo que nos lleva sencillamente a esto:\n\\[\nPr(vampiro|positivo) = \\frac{95}{1094} ≈ 0.087\n\\]\nEsta es exactamente la misma respuesta de 8.7% que encontramos antes. Pero no tuvimos que recordar la ´“formula mágica” de Bayes, nada más tuvimos que contar y pensar con calma.\n\n\n\nEsta forma de presentar el problema mediante el “conteo de los actores” en lugar de recurrir a probabilidades, suele denominarse formato de frecuencia o frecuencias naturales.\nLas razones propuestas para explicar el por qué el formato de frecuencia ayuda a la gente a intuir el enfoque correcto siguen siendo polémicas. Podría ser que de entrada sólo podemos encontrarnos con conteos en el mundo real. Quizás sea cierto que nadie ha visto nunca una probabilidad andando por ahí. Independientemente de la explicación de este fenómeno, podemos explotarlo.\nLos eventos muestreados en el análisis de las distribuciones de probabilidades de modelos estadísticos en algún análisis de datos, son los valores de los parámetros. La mayoría de los parámetros no tienen una “materialización” empírica exacta.\nEl formalismo bayesiano trata las distribuciones de los parámetros como una plausibilidad relativa, no como un proceso aleatorio que ocurre en el mundo físico. En cualquier caso, la aleatoriedad es siempre una propiedad de la información, nunca del mundo real.\n\n\n\nEl ejemplo del vampirismo que acabamos de ver tiene la misma estructura lógica que muchos problemas de detección considerando que:\n\nHay algún estado binario al que no tenemos acceso.\nObservamos un indicio imperfecto del estado oculto.\n(Deberíamos/podríamos) usar el teorema de Bayes para deducir lógicamente el impacto del indicio en nuestra incertidumbre (aunque ve lo que salió en el periódico)\n\nLa inferencia científica puede enmarcase en términos similares:\n\nUna hipótesis es verdadera o falsa, pero no podemos saberlo;\nObtenemos un indicio estadístico de la falsedad de la hipótesis;\nDebemos/podemos utilizar el teorema de Bayes para deducir lógicamente el impacto del indicio en el estado de la hipótesis.\n\nEs el tercer paso el que casi nunca se hace. Sin debatir por lo pronto si debemos o no usar a Bayes, consideremos por un momento la idea como un ejemplo de juguete.\n\n\n\n\n\nSupongamos que la probabilidad de obtener un hallazgo positivo, cuando la hipótesis postulada es cierta, es \\({Pr(señal~detectada|verdadero) =Pr(H|V) = 0.95}\\).\nEse es lo que se suele llamar la potencia de la prueba.\n\n\n\nSupongamos que la probabilidad de un hallazgo positivo, cuando una hipótesis es falsa, es \\({Pr(señal~detectada|falso) = Pr(H|F) = 0.05}\\).\nEsa es la tasa de falsos positivos, se trata del, digamos 5%, de la prueba de significancia que usamos convencionalmente.\n\n\n\nFinalmente, tenemos que establecer la tasa base con la que ocurren las hipótesis que son verdaderas. Supongamos, por ejemplo, que 1 de cada 100 hipótesis resulta ser verdadera. Entonces \\({Pr(verdadero) = P(V) = 0.01}\\).\nEn realidad nadie conoce este valor ni se ve posible conocerlo, pero la historia de la ciencia sugiere que es pequeño.\n\n\n\nPara averiguar esto, calculamos la componente a posteriori:\n\\[\nPr(detectada|Hipótesis) = \\frac{Pr(Hipótesis|detectada) Pr(detectado)} {Pr(Hipótesis)} = \\\\\n\\\\\n\\frac{Pr(H|V) Pr(V)} {Pr(H|V) Pr(V) + Pr(H|F) Pr(F)}\n\\\\\n\\]\n\n\n\n\n\n\n\n\nComo podemos ver, al substituir los valores imaginados, encontramos que la respuesta es aproximadamente \\({Pr(V|H) = 0.16}\\).\nAsí que un resultado positivo corresponde a un 16% de probabilidad de que la hipótesis sea cierta.\nEste es el mismo fenómeno de baja tasa de base que se aplica en las pruebas médicas (y también en nuestro ejemplo de vampiros).\n\n\n\n\n\n\n¿Podremos mejorar la práctica científica?\n\n\n\n\n\n\n\n\n\n\n\n\nExperimentos pensados\n\n\n\n\n\nUna manera de explorar lo que puede pasar en distintos escenarios, asumiendo un razonamiento específico, nos da oportunidad de valorar la utilidad de hacer un escript, algoritmo o programa. Así podemos automatizar una tarea repetitiva y potencialmente aburrida para ver las implicaciones de la idea en todo tipo de situaciones. Qizás podríamos considerarlo semejante a lo qe Einstein llamaba experimento pensado. Veamos como hacerlo.\n\n\n\n\n\n\n\n\nHemos definido un programa como una función en R. Esta función puede tomar datos y procesarlos de acuerdo con la lógica que le hemos especificado.\nAhora podemos experimentar para tener una idea aproximada de lo que está pasando.\n\nElegimos una serie de valores de interés (cada uno sería un escenario)\n\n\n\n\n\n\n\n\n\n\nPreparamos un espacio en la memoria para anotar los resultados.\n\n\n\n\n\n\n\n\n\n\nVeamos los resultados del experimento con ayuda de una gráfica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Tú que piensas?\n\n\n\n\n\nUna investigación muy exigente que reduce la detección de falsos positivos a 1%, nos permite llevar la probabilidad posterior de descubrimientos exitosos hasta 0.5.\nApenas tan buena como el lanzamiento de una moneda.\nPodemos pensar que lo que hemos hecho hasta aquí es prácticamente un juego, pero ¿qué tan cercano podría ser a lo que ocurre en la vida real? y si fuera una razonable aproximación ¿a qué nos conduce?\nQuizás nos sugiera que lo más importante es mejorar la tasa base, \\({Pr(V)}\\), y eso requiere pensar mejor, no hacer muchas más pruebas o incluso ponerse exageradamente quisquilloso\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFuente: Stoddart, C. (2016). Is there a reproducibility crisis in science?. Nature\n     \n\n\n\n\n\n\n¿Ha mejorado la situación?\n\n\n\n\n\n\n\n\n\n\n\nLa falta de reproducibilidad de los experimentos se traduce en artículos que son cuestionados y se ven forzados a retirarse. Claro, no es la única razón para retirar un artículo, pero si la más frecuente. A fines del año pasado salió esta noticia en Nature.\n\n\n\n\n \n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nSin duda la forma como hemos optado por hacer ciencia está teniendo problemas que debemos afrontar. La cuestión es que podemos o más bien que debemos hacer para enfrentarlos productivamente y así producir una ciencia mejor.\n\n\n\n\n\n\nBuscar la verdad\n\n\n\n\n\n\n\n\n\n\n\n\nLa búsqueda de la verdad siempre debería prevalecer sobre el deseo defensivo del ego de tener la razón. Esto no es fácil, porque a la mayoría de la gente le resulta difícil admitir que se equivoca. Y es precisamente por esto por lo que la ciencia resulta tan liberadora. Provee un marco de trabajo para la autocorrección, porque el conocimiento científico siempre es provisional. Un hecho científico aceptado hoy puede ser refutado mañana. Por tanto, el método científico engendra humildad epistemológica.\n\n\n\n \n\n\n\n\n\n\nGad Saad"
  },
  {
    "objectID": "posts/01-concepto-y-modelos-experimentar/index.html#estadística-para-mejorar-la-ciencia",
    "href": "posts/01-concepto-y-modelos-experimentar/index.html#estadística-para-mejorar-la-ciencia",
    "title": "Conceptos y Modelos en la experimentación",
    "section": "",
    "text": "El ejemplo del vampirismo que acabamos de ver tiene la misma estructura lógica que muchos problemas de detección considerando que:\n\nHay algún estado binario al que no tenemos acceso.\nObservamos un indicio imperfecto del estado oculto.\n(Deberíamos/podríamos) usar el teorema de Bayes para deducir lógicamente el impacto del indicio en nuestra incertidumbre (aunque ve lo que salió en el periódico)\n\nLa inferencia científica puede enmarcase en términos similares:\n\nUna hipótesis es verdadera o falsa, pero no podemos saberlo;\nObtenemos un indicio estadístico de la falsedad de la hipótesis;\nDebemos/podemos utilizar el teorema de Bayes para deducir lógicamente el impacto del indicio en el estado de la hipótesis.\n\nEs el tercer paso el que casi nunca se hace. Sin debatir por lo pronto si debemos o no usar a Bayes, consideremos por un momento la idea como un ejemplo de juguete.\n\n\n\n\n\nSupongamos que la probabilidad de obtener un hallazgo positivo, cuando la hipótesis postulada es cierta, es \\({Pr(señal~detectada|verdadero) =Pr(H|V) = 0.95}\\).\nEse es lo que se suele llamar la potencia de la prueba.\n\n\n\nSupongamos que la probabilidad de un hallazgo positivo, cuando una hipótesis es falsa, es \\({Pr(señal~detectada|falso) = Pr(H|F) = 0.05}\\).\nEsa es la tasa de falsos positivos, se trata del, digamos 5%, de la prueba de significancia que usamos convencionalmente.\n\n\n\nFinalmente, tenemos que establecer la tasa base con la que ocurren las hipótesis que son verdaderas. Supongamos, por ejemplo, que 1 de cada 100 hipótesis resulta ser verdadera. Entonces \\({Pr(verdadero) = P(V) = 0.01}\\).\nEn realidad nadie conoce este valor ni se ve posible conocerlo, pero la historia de la ciencia sugiere que es pequeño.\n\n\n\nPara averiguar esto, calculamos la componente a posteriori:\n\\[\nPr(detectada|Hipótesis) = \\frac{Pr(Hipótesis|detectada) Pr(detectado)} {Pr(Hipótesis)} = \\\\\n\\\\\n\\frac{Pr(H|V) Pr(V)} {Pr(H|V) Pr(V) + Pr(H|F) Pr(F)}\n\\\\\n\\]\n\n\n\n\n\n\n\n\nComo podemos ver, al substituir los valores imaginados, encontramos que la respuesta es aproximadamente \\({Pr(V|H) = 0.16}\\).\nAsí que un resultado positivo corresponde a un 16% de probabilidad de que la hipótesis sea cierta.\nEste es el mismo fenómeno de baja tasa de base que se aplica en las pruebas médicas (y también en nuestro ejemplo de vampiros).\n\n\n\n\n\n\n¿Podremos mejorar la práctica científica?\n\n\n\n\n\n\n\n\n\n\n\n\nExperimentos pensados\n\n\n\n\n\nUna manera de explorar lo que puede pasar en distintos escenarios, asumiendo un razonamiento específico, nos da oportunidad de valorar la utilidad de hacer un escript, algoritmo o programa. Así podemos automatizar una tarea repetitiva y potencialmente aburrida para ver las implicaciones de la idea en todo tipo de situaciones. Qizás podríamos considerarlo semejante a lo qe Einstein llamaba experimento pensado. Veamos como hacerlo.\n\n\n\n\n\n\n\n\nHemos definido un programa como una función en R. Esta función puede tomar datos y procesarlos de acuerdo con la lógica que le hemos especificado.\nAhora podemos experimentar para tener una idea aproximada de lo que está pasando.\n\nElegimos una serie de valores de interés (cada uno sería un escenario)\n\n\n\n\n\n\n\n\n\n\nPreparamos un espacio en la memoria para anotar los resultados.\n\n\n\n\n\n\n\n\n\n\nVeamos los resultados del experimento con ayuda de una gráfica.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Tú que piensas?\n\n\n\n\n\nUna investigación muy exigente que reduce la detección de falsos positivos a 1%, nos permite llevar la probabilidad posterior de descubrimientos exitosos hasta 0.5.\nApenas tan buena como el lanzamiento de una moneda.\nPodemos pensar que lo que hemos hecho hasta aquí es prácticamente un juego, pero ¿qué tan cercano podría ser a lo que ocurre en la vida real? y si fuera una razonable aproximación ¿a qué nos conduce?\nQuizás nos sugiera que lo más importante es mejorar la tasa base, \\({Pr(V)}\\), y eso requiere pensar mejor, no hacer muchas más pruebas o incluso ponerse exageradamente quisquilloso"
  },
  {
    "objectID": "posts/01-concepto-y-modelos-experimentar/index.html#crisis-de-reproducibilidad",
    "href": "posts/01-concepto-y-modelos-experimentar/index.html#crisis-de-reproducibilidad",
    "title": "Conceptos y Modelos en la experimentación",
    "section": "",
    "text": "Fuente: Stoddart, C. (2016). Is there a reproducibility crisis in science?. Nature\n     \n\n\n\n\n\n\n¿Ha mejorado la situación?\n\n\n\n\n\n\n\n\n\n\n\nLa falta de reproducibilidad de los experimentos se traduce en artículos que son cuestionados y se ven forzados a retirarse. Claro, no es la única razón para retirar un artículo, pero si la más frecuente. A fines del año pasado salió esta noticia en Nature.\n\n\n\n\n \n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nSin duda la forma como hemos optado por hacer ciencia está teniendo problemas que debemos afrontar. La cuestión es que podemos o más bien que debemos hacer para enfrentarlos productivamente y así producir una ciencia mejor.\n\n\n\n\n\n\nBuscar la verdad\n\n\n\n\n\n\n\n\n\n\n\n\nLa búsqueda de la verdad siempre debería prevalecer sobre el deseo defensivo del ego de tener la razón. Esto no es fácil, porque a la mayoría de la gente le resulta difícil admitir que se equivoca. Y es precisamente por esto por lo que la ciencia resulta tan liberadora. Provee un marco de trabajo para la autocorrección, porque el conocimiento científico siempre es provisional. Un hecho científico aceptado hoy puede ser refutado mañana. Por tanto, el método científico engendra humildad epistemológica.\n\n\n\n \n\n\n\n\n\n\nGad Saad"
  },
  {
    "objectID": "posts/01-concepto-y-modelos-experimentar/index.html#anova",
    "href": "posts/01-concepto-y-modelos-experimentar/index.html#anova",
    "title": "Conceptos y Modelos en la experimentación",
    "section": "ANOVA",
    "text": "ANOVA\n\nConceptos básicos (modelo de una vía, un criterio, completamente aleatorizado)\nHay situaciones en las que la información que tenemos para predecir una respuesta la tenemos en forma cualitativa, incluso la presencia o ausencia de una categoría. Una variable categórica es una medición discreta y las clases no tienen ningún orden particular. Por ejemplo, consideremos de nuevo las diferentes especies en los datos de energía láctea. Algunas de ellas son simios, mientras que otras son monos del Nuevo Mundo. Podríamos preguntarnos cómo deberían variar las predicciones cuando la especie es un simio en lugar de un mono. El grupo taxonómico es una variable categórica, porque ninguna especie puede ser mitad simio y mitad mono (discreción), y no hay ningún sentido en el que uno sea más grande o más pequeño que el otro (desorden). Otros ejemplos comunes de variables categóricas son:\n\nSexo: macho, hembra\nEstado de desarrollo: lactante, juvenil, adulto\nRegión geográfica: África, Europa, Melanesia\n\nAlgunos de ustedes ya sabrán que variables como esta, llamadas rutinariamente factores, pueden ser fácilmente incluidas en los modelos lineales. Pero lo que no resulta tan intuitivo es la forma cómo se representan estas variables en un modelo. El ordenador hace todo el trabajo por nosotros, ocultando la maquinaria.\nLa hipótesis nula en un análisis de la varianza tipo I común es:\n\\[\nH_0: m_1 = m_2 = m_3 = ... = m_k\n\\]\n\n\n\n\n\n\n¿Cómo es que esta hipótesis se pone a prueba en un ANDEVA?\n\n\n\nPor cierto esta es una prueba “omnibus”, es decir ¡prueba todo (la igualdad de todas las medias) de un jalón!\n\n\nPara ver como es que opera el anova veamos el ejemplo que sigue. Considera un solo factor, “f”, con dos niveles.\n\n\n\n\n\n\n\n\nPongamos estos datos en una gráfica simple, sgún el orden en el que fueron obtenidas las mediciones. Lo primero que haremos es definir la tabla de datos anova.data, como espacio de trabajo. Haremos esto con la función attach(). Esto hace que las variables contenidas en la tabla se puedan llamar directamente sin tener que anteponer el nombre de la estructura que las contiene. Esto es conveniente, pero si olvidamos regresar al espacio general de trabajo, con la función detach(), podemos encontrarnos con situaciones algo extrañas. En caso de que eso ocurra, resulta útil la función search(), que muestra los espacios de trabajo activos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Qué muestra esta gráfica? ¿a que equivale la suma de los trazos verticales?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAhora incorporemos la información del factor f. Para esto hay que calcular los promedios de “y” que corresponden a los niveles de f\n\n\n\n\n\n\n\n\nGrafiquemos esta nueva estructura de datos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Qué muestra esta gráfica? ¿a que equivale la suma de los trazos verticales?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSi las dos medias fueran iguales ¿cómo compararían estas dos gráficas?\n\n\n\n\n\n\n¿Qué interpretación tiene la diferencia entre las dos sumas mencionadas arriba?\n\n\n\n\n\nEsta diferencia se asocia con la siguiente gráfica:\n\n\n\n\n\n\n\n\nAhora la suma de estas líneas verticales es\n\n\n\n\n\n\n\n\n\n\n\nEstas tres formas de calcular las distancias entre datos y promedios se asocia con fuentes de variación\n\nvariaciones o error total\nvariaciones o error residual (componente aleatorio/efecto aleatorio)\nvariaciones o error del modelo (componente sistemático/efecto fijo)\n\nEn el cuadro de análisis de varianza se suele etiquetar a los componentes de error de acuerdo con su fuente. Se les acompaña con los grados de libertad, la suma de cuadrados de las distancias que mostré en las tres gráficas anteriores y luego los llamados cuadrados medios. Para referencia podemos pedrle a R que nos reporte el cuadro de ANOVA de este modelo.\n\n\n\n\n\n\nAnaliza la correspondencia entre los valores y las gráficas que vimos arriba con lo que reporta R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDisposición de tratamientos e intereses sobre los factores\n\nEfectos fijos\nQuizás la forma más simple de identificar las variables explicativas que tienen efectos fijos es pensar en ellas como variables cuyos niveles identifican en forma completa las condiciones de interés para el investigador. Por ejemplo, en el caso de un experimento que analiza el desempeño de larvas de mariposa que toman una dieta rica en proteinas y al mismo tiempo están expuestas a la presencia o no de un alcaloide. Estamos interesados precisamente en esas dietas y en la presencia o no del alcaloide. Estos dos factores son fijos. Es el tipo de variables que normalmente consideramos en nuestros objetivos de investigación. Se asume que su identificación y definición es completa, se asume que no hay más niveles de interés que los definidos y por lo tanto el modelo resultante no puede utilizarse para predecir fuera del ámbito de esas definiciones.\n\n\nEfectos aleatorios\nLas variables de efectos aleatorios surgen cuando se considera que el factor considerado no es sino una muestra de los posibles resultados que se pueden obtener de muestrear la condición que caracteriza el factor. Por ejemplo, si en un experimento para explorar la germinación de Bouteloua gracilis bajo distintas condiciones de temperatura en campo, se distribuyen las semillas en varios sitios de una zona de interés. Los aprecia que hay básicamente dos tipos de ambiente, suelos arenosos y suelos con algo de grava, así que se eligen 5 sitios en cada condición, y en cada uno de ellos se ponen a prueba dos tratamientos, “pisoteo por ganado” y “sin pisoteo por ganado”. Los 10 sitios elegidos estarían definidos como de efectos aleatorios, pues podemos ver que los niveles elegidos son en realidad una muestra de las posibles condiciones que prevalecen en la zona de estudio. Además, claramente el interés de la predicción es ser generalizable para toda la zona. A veces podemos pensar en esta forma de proceder como equivalente a un muestreo estratificado, en este caso, los tipos de ambiente son los estratos. No es el caso del tratamiento pisoteo. aprovechando podemos ver que en este experimento tendremos un mínimo de 4 combinaciones experimentales, y que ese arreglo mínimo se repetirá 5 veces, así que reqeriremos 20 unidades experimentales para realizar el estudio.\n::: {.callout-tip title=“¿Puedes darnos un ejemplo en el que distingas entre efectos fijos y aleatorios?”\n\n\n\n\n\n\n\n\nAnidamiento vs. cruzamiento\nLa anidación o el cruzamiento es otra característica de los datos, o más bien del diseño experimental. Hablamos de que un conjunto de variables están cruzadas en un diseño experimental cuando todos los posibles niveles de las variables están expuestas por igual entre ellas. Podríamos decir que las variables se combinan de “igual a igual”. Es decir, podemos tener tantas posibles combinaciones de las variables como el producto del numero de niveles que tengan. En el caso del experiment de Bouteloua, el experimento sugiere que los sitios y los tratamientos están “cruzados”, de ahí que tengamos necesidad de disponer por lo menos de 4 unidades experimentales.\nEl ejemplo de escuelas que ilustro a continuación debe ayudar a entender mejor estos conceptos. Si las clases son iguales para todas las escuelas, nos estaríamos refiriendo a algo así:\n\nEsto significa que cada clase se imparte por igual y en las mismas condiciones a cada escuela. Algo difícil de imaginar, ¡pero quizás no en los tiempos de la COVID-19!. Este es un diseño cruzado (algunos también podrían llamarlo afiliación múltiple). En R y con las funciones que ajustan modelos estadísticos lineales (lm() y glm()) se produce mediante el operador *.\nEl arreglo anidado se produce cuando las unidades experimentales están subordinadas a algún criterio de clasificación. Un factor B está anidado en otro factor A cuando cada nivel del factor B aparece asociado a un único nivel del factor A (los niveles de B están subordinados a los de A). Aquí tenemos clases anidadas en escuelas, lo cual es un escenario familiar.\n\nEl punto importante aquí es que, entre cada escuela, las clases tienen el mismo identificador, aunque sean distintas si están anidadas. La clase 1 aparece en la escuela 1, la escuela 2 y la escuela 3. Sin embargo, si los datos están anidados, la clase 1 en la escuela 1 no es la misma unidad de medida que la clase 1 en la escuela 2 y la escuela 3.\nNo es posible saber, simplemente inspeccionando los datos, si tenemos efectos aleatorios anidados o cruzados. Esto sólo puede determinarse con el conocimiento de los datos y el diseño experimental. Debido a esto, es muy importante especificar con suficiente claridad el diseño experimental incluyendo las operaciones involucradas para ponerlo en práctica, para poder construir correctamente el modelo estadístico correspondiente, ya que dependiendo de la naturaleza de las variables (fija o aleatoria), los modelos producirán resultados diferentes.\nEl concepto de variables aleatorias no es fácil de comprender, por lo que no hay que preocuparse demasiado por entenderlo completamente en este momento. También es útil tener en cuenta que el hecho de que una variable se considere fija o aleatoria en cierto grado dependerá de la interpretación de la persona que diseña el experimento y realiza el análisis. En R, la operación para incluir efectos anidados es el operador /.\nEn forma específica las interacciones derivadas de cruzamiento se pueden anotar en un modelo como a:b y un anidamiento a %in% b\nNaturalmente podemos encontrar situaciones en las que el experimento combina efectos aleatorios y fijos. Naturalmente, tal diseño se denomina de efectos mixtos."
  },
  {
    "objectID": "posts/01-concepto-y-modelos-experimentar/index.html#disposición-de-tratamientos-e-intereses-sobre-los-factores",
    "href": "posts/01-concepto-y-modelos-experimentar/index.html#disposición-de-tratamientos-e-intereses-sobre-los-factores",
    "title": "Conceptos y Modelos en la experimentación",
    "section": "Disposición de tratamientos e intereses sobre los factores",
    "text": "Disposición de tratamientos e intereses sobre los factores\n\nEfectos fijos\nQuizás la forma más simple de identificar las variables explicativas que tienen efectos fijos es pensar en ellas como variables cuyos niveles identifican en forma completa las condiciones de interés para el investigador. Por ejemplo, en el caso de un experimento que analiza el desempeño de larvas de mariposa que toman una dieta rica en proteinas y al mismo tiempo están expuestas a la presencia o no de un alcaloide. Estamos interesados precisamente en esas dietas y en la presencia o no del alcaloide. Estos dos factores son fijos. Es el tipo de variables que normalmente consideramos en nuestros objetivos de investigación. Se asume que su identificación y definición es completa, se asume que no hay más niveles de interés que los definidos y por lo tanto el modelo resultante no puede utilizarse para predecir fuera del ámbito de esas definiciones.\n\n\nEfectos aleatorios\nLas variables de efectos aleatorios surgen cuando se considera que el factor considerado no es sino una muestra de los posibles resultados que se pueden obtener de muestrear la condición que caracteriza el factor. Por ejemplo, si en un experimento para explorar la germinación de Bouteloua gracilis bajo distintas condiciones de temperatura en campo, se distribuyen las semillas en varios sitios de una zona de interés. Los aprecia que hay básicamente dos tipos de ambiente, suelos arenosos y suelos con algo de grava, así que se eligen 5 sitios en cada condición, y en cada uno de ellos se ponen a prueba dos tratamientos, “pisoteo por ganado” y “sin pisoteo por ganado”. Los 10 sitios elegidos estarían definidos como de efectos aleatorios, pues podemos ver que los niveles elegidos son en realidad una muestra de las posibles condiciones que prevalecen en la zona de estudio. Además, claramente el interés de la predicción es ser generalizable para toda la zona. A veces podemos pensar en esta forma de proceder como equivalente a un muestreo estratificado, en este caso, los tipos de ambiente son los estratos. No es el caso del tratamiento pisoteo. aprovechando podemos ver que en este experimento tendremos un mínimo de 4 combinaciones experimentales, y que ese arreglo mínimo se repetirá 5 veces, así que reqeriremos 20 unidades experimentales para realizar el estudio.\n::: {.callout-tip title=“¿Puedes darnos un ejemplo en el que distingas entre efectos fijos y aleatorios?”"
  },
  {
    "objectID": "posts/01-concepto-y-modelos-experimentar/index.html#dag-de-un-diseño-experimental-simple",
    "href": "posts/01-concepto-y-modelos-experimentar/index.html#dag-de-un-diseño-experimental-simple",
    "title": "Conceptos y Modelos en la experimentación",
    "section": "DAG de un diseño experimental simple",
    "text": "DAG de un diseño experimental simple\nUn experimento con asignación de tratamientos en forma completamente al azar, garantiza al máximo posile las vías de influencias ocultas o inadvertidas. Por diseño, la respuesta de las unidades experimentales, Y, al tratamiento T sólo tiene a la asignación aleatoria A como única causa que antecede al tratamiento. Esto lo podemos representar con el diagrama acíclico dirigido, DAG, siguiente:\n\\[\n\\fbox{A} \\rightarrow T \\rightarrow Y\n\\]\nEn este DAG, el marco que rodea a la A indica aleatorización, y como sugiere el diagrama, es la única causa que actua sore el tratamiento. Si hubiera una vía de influencia alternativa (backdoor), a través de alguna tercera variable como podría ser en un caso de germinación de semillas, la luminosidad del sitio o el grado de humendad en el sustrato; entonces, la aleatorización no sería el único factor que influiría sore el tratamiento. El recuadro alrededor de A (el proceso de aleatorización) indica que no existen otros factores actuando sobre T, es decir, A es una influencia puramente estocástica. Esta idea y el proceso de realización de un experimento controlado con aleatorización, da cuenta con toda claridad del valor de esta forma de realizar estudios para desentrañar relaciones de causalidad.\nEste DAG nos conduce al modelo linear siguiente\n\\[\ny = \\mu + T +  \\varepsilon\n\\]\nEn donde y son las mediciones de la variable Y en respuesta al efecto de T, \\(\\mu\\) es un valor de referencia general (tradicionalmente la media general de la variale Y, aunque puede elegirse cualquier otro valor de referencia que convenga al estudio) y la épsilon da cuenta del efecto aleatorio inducido por \\(\\fbox{A}\\), por lo que es necesario postular una distribución de probabilidades apropiada para caracterizar su comportamiento."
  },
  {
    "objectID": "posts/01-concepto-y-modelos-experimentar/index.html#la-tradición-de-prueba-de-hipótesis",
    "href": "posts/01-concepto-y-modelos-experimentar/index.html#la-tradición-de-prueba-de-hipótesis",
    "title": "Conceptos y Modelos en la experimentación",
    "section": "La tradición de prueba de hipótesis",
    "text": "La tradición de prueba de hipótesis\nEl planteamiento de la prueba de significancia estadística de hipótesis se pueden encontrar ya en el siglo XIX, su formalización teórica realmente ocurre en los años 20 y 30 del siglo XX con las publicaciones de Sir Ronald Fisher, Jerzy Neyman y Egon Pearson.\nEntre estos autores existieron diferencias filosóficas y conceptuales entre sus planteamientos y posturas. En special Fisher y Neyman sostuvieron acres debates que sólo se interrumpieron con el fallecimiento de Fisher en 1962. No obstante el debate continua hasta hoy.\nEl resultado es que el uso actual de la prueba estadística de hipótesis se ha conformado como un extraño híbrido surgido de una mezcla más o menos ecléctica de las dos formas de pensar y no tanto una teoría coherente sobre la prueba de hipótesis.\n\nEl procedimiento\nEl objetivo de una prueba de significancia es hacer inferencias sobre un parámetro que el investigador concibe asociado a un atributo numérico relevante de la población que define su objetivo de investigación. El procedimiento utiliza como base los datos de una muestra extraída de esa población. El enfoque se opera específicamente como un instrumento para excluir un valor o una gama de valores específicos como plausibles para el parámetro.\n\nEl paso a paso del formalismo de prueba de hipótesis\n\nConstruir un modelo estadístico. Se trata de un conjunto de supuestos sobre las variables de interés.\nEspecificar la hipótesis nula.\nDefinir un estadístico de contraste (frecuentemente llamada “la prueba estadística”).\nIdentificar la distribución del estadístico de contraste bajo los supuestos del modelo.\nCalcular, bajo el supuesto de la hipótesis nula, el valor del estadístico de contraste en la muestra observada.\nCalcular la probabilidad de tener un valor del estadístico como el resultante o un valor más extremo en la distribución de referencia (el famosos valor p).\nAceptar o rechazar la hipótesis nula. Si el valor p es menor que el criterio α de significancia (especificado a priori), se rechaza la hipótesis nula, en el caso contrario se acepta o por lo menos no se rechaza (por lo pronto).\n\nRechazar la hipótesis nula es algo que quizás produce poca tensión emocional, quizás hasta un alivio, finalmente, el investigador sospecha (desea mostrar) que lo interesante está en otra parte, en su juego de hipótesis alternativas.\n\n\n\n¿Es este procedimiento afín al refutacionismo Popperiano?.\nPara interpretar correctamente un valor p se necesita tener claro que se opera dentro de una marco conceptual frecuentista. Esto lleva a que se conciba a los parámetros del modelo estadístico como constantes en la población objetivo (valor fijo que nunca se conoce en realidad).\nAdemás se asume que, al menos conceptualmente, sería posible repetir el experimento un número infinito de veces. También se asume que siempre se está muestreando la misma población objetivo (universo muestral) así que los parámetros tienen el mismo valor, pero las muestras fluctúan aleatoriamente.\nBajo estos supuestos es aceptable considerar que el estadístico de prueba se distribuye de acuerdo con el modelo de probabilidades propuesto para construir el contraste y por lo tanto da cuenta de las variaciones esperadas entre las diferentes repeticiones del experimento.\nEn la aproximación tradicional a la contrastación estadística de hipótesis (frecuentista) se parte de la formulación de proposiciones hipotéticas que son descritas con referencia a alguna distribución de probabilidades.\nEn este marco conceptual, un componente es la llamada hipótesis nula (\\(H_{0}\\)). Se concibe como un planteamiento que asume la ausencia de efecto de los “factores explicativos”.\nEn contraste se propone una o más hipótesis alternativas (\\(H_{1...n}\\)), en las que se valora algún o algunos efectos de los “factores explicativos”. La proposición hipotética que hacemos se traduce en valores que podemos comparar con un conjunto de valores a los que consideramos observados. La diferencia entre estos dos conjuntos de valores nos permiten valorar la factibilidad de nuestra proposición.\n\nEn la práctica, suele ocurrir que se concentre la atención en la hipótesis nula expresada con la gran simplicidad que implica la ausencia de efectos y se proceda con menor rigurosidad el análisis de la hipótesis alternativa, la que suele procesarse en forma más bien exploratoria mediante procedimientos de comparaciones múltiples.\n\n\nCríticas\nEntre las críticas que se han hecho al procedimiento clásico de prueba de hipótesis está la que señala que el valor p, al excluir el valor de cero como valor plausible para el parámetro, no aporta información completa sobre los valores que sí son plausibles. Esto implica que la significancia estadística no implica relevancia práctica.\n\n\n¿Cómo interpretas esta afirmación?\nEn el mismo razonamiento, un “valor de p extremadamente significativo” no hace otra cosa que excluir el cero como valor plausible para el parámetro no precisamente sobre la calidad del hallazgo.\nOtra crítica señala que interpretar el valor p en términos de evidencia en contra de la hipótesis nula (siguiendo el pensamiento de Fisher) o la plausibilidad de que la hipótesis nula sea falsa a veces se expresan equivocadamente como la probabilidad de que la hipótesis nula sea falsa en consideración de la evidencia (E) disponible. Al plantearlo así, formalmente se enuncia como \\(P(H_{0}|E)\\). Pero esto no es apropiado, formalmente resulta ser una inconsistencia en la lógica del planteamiento.\n\n\n¿Puedes reconocer esta inconsistencia?\nLa inconsistencia está en que, en primer lugar como dije arriba, el valor p se define dentro del marco frecuentista y se concibe que los parámetros son valores constantes, aunque desconocidos (¡supuesto estadístico de efectos fijos!). No se trata de los parámetros de alguna distribución de probabilidades (observada o no). Por tanto, no tiene sentido asignar probabilidades a los distintos valores estimados del parámetro.\nAdemás, el valor p se calcula bajo el supuesto de que la hipótesis nula es cierta; esto hace imposible, por construcción, interpretarlo como la probabilidad de que la hipótesis alternativa sea cierta. La probabilidad a la que se refiere el valor p guarda más relación con la probabilidad inversa, \\(P(E|H{0})\\). ¿Qué tan probable sería tener una muestra como la que tenemos enfrente, si la hipótesis considerada fuera cierta?. Esto se conoce como la verosimilitud, es decir, la probabilidad de observar los datos que se han obtenido en un estudio suponiendo que los atributos del modelo fueran ciertos. La verosimilitud valora a la muestra como un resultado condicional a los supuestos hechos en el modelo estadístico y en este caso, la hipótesis nula.\nSin embargo, la probabilidad que realmente interesa -por ejemplo, al investigador de nuestro ejemplo- es la anteriormente mencionada \\(P(H_{0}|E)\\). Aunque no está definida dentro del marco frecuentista, en el marco Bayesiano sí se define. Las probabilidades \\(P(E|H_{0})\\) y \\(P(H_{0}|E)\\) no son iguales.\n\n\n¿Recuerdas qué representa cada uno de ellas? ¿cuál es la relación entre ambas?\nOtra crítica interesante surge de la llamada paradoja de Lindley (1957), quien mostró, con una formulación Bayesiana, que existe la posibilidad de tener datos congruentes con rechazar una hipótesis nula con un bajo valor p y que al mismo tiempo llevan a una probabilidad posterior alta.\nEncontró que es perfectamente posible, a partir de los mismos datos E, obtener al mismo tiempo una \\(P(E|H_{0})\\) = 0.05 (baja probabilidad de obtener una muestra como la que se observó, si \\(H_{0}\\) fuera cierta) y \\(P(H_{0}|E)\\) = 0.95 (fuerte evidencia en favor de \\(H_{0}\\)). Este resultado contradictorio permite ver lo cuestionable que resulta interpretar el valor p como evidencia en contra de la hipótesis nula.\nSe ha contrargumentado que la paradoja requiere muestras grandes para manifestarse, y se oponen a los supuestos adicionales que requiere el análisis Bayesiano. Se ha defendido que bajo condiciones razonables, un bajo valor p generalmente implica una baja probabilidad posterior, es decir, poca evidencia para la hipótesis nula. Sin embargo, a pesar de esta defensa al enfoque clásico, se ha encontrado que los valores p sistemáticamente sobrestiman la evidencia en contra de la hipótesis nula.\nEn resumen, el valor mismo de p, resultado de una prueba clásica de hipótesis, no aporta mucha información de interés para los investigadores. En caso de optar por la hipótesis alternativa con base en la p, no se favorece llegar a ninguna conclusión sustancial sobre posibles explicaciones alternativas, lo único que queda claro es que la nula probablemente es falsa.\nPara que quede claro, hay que insistir en que si el valor p es juzgado significativo, únicamente nos inclina a excluir un solo valor como estimador plausible para el parámetro. Peor aún, el significado de plausible en la última expresión tiene una relación nebulosa con la probabilidad que sí le interesa a los investigadores: la probabilidad posterior de que la hipótesis nula sea cierta a la luz de la evidencia recopilada \\(P(H_{0}|E)\\).\n\n\n\n\n\n\n¿Qué piensas de la paradoja de Lindley y sus implicaciones\n\n\n\n\n\n\n\n\nRemedios y alternativas para la prueba estadística de la hipótesis nula\nPara enfrentar algunos de los inconvenientes del enfoque clásico de prueba de hipótesis se ha recomendado ahora sustituir el valor p por un intervalo de confianza que abarca un conjunto de valores que permiten valorar si es razonable rechazar la hipótesis nula y además, en caso contrario proporciona una gama de valores que caracterizan al parámetro, lo que resulta de mucho interés.\nLa práctica de presentar intervalos de confianza, posiblemente en conjunto con p, constituye una respuesta a la crítica de que sólo se excluye un valor como valor plausible para el parámetro. Además, hacer esto proporciona información sobre significancia. Si el intervalo no incluye el valor de cero, entonces se declararía el resultado como estadísticamente significativo. El intervalo informa también sobre el posible tamaño del efecto.\nA la luz de las críticas, Muchos autores ven necesario actualmente adoptar el marco Bayesiano para enfrentar las deficiencias del enfoque clásico.\n\n\nPotencia de la prueba\nEn la práctica, la potencia de la prueba depende del grado de dispersión en los datos. Si se está asumiendo un modelo de probabilidades Gaussiano (distribución normal), el factor de dispersión o escala se relaciona con la varianza. Por lo tanto, es usual notar que el mismo cálculo del error estandar, \\(s_{e}=\\frac{\\sigma}{\\sqrt{n}}\\), sugiere la solución.\n\n\nSe puede incrementar el tamaño de muestra, n,\n\n\n\n\n\n\n\n¿Por qué funcionaría esto?\n\n\n\n\n\n\n\nAumentar la precisión con la que se estima \\(\\sigma^2\\),\n\n\n\n\n\n\n\n¿Cómo se puede hacer esto?\n\n\n\n\n\n\nEs interesante apreciar, que la búsqueda de un tamaño de muestra apropiado para un estudio que estemos planeando, se puede lograr muy eficazmente haciendo simulaciones como las que hemos estado viendo en este bloque del curso. A través de este camino y haciendo el esfuerzo de especificar hipótesis alternativas relevantes se pueden resolver preguntas como:\n\n¿Cuál es el tamaño de muestra necesario para detectar una cierta diferencia en lo que medimos?\n¿Cuál es la diferencia detectable dada una n o una potencia de la prueba (\\(1-\\beta\\))?\n¿Cuál es la potencia (\\(1-\\beta\\)) dado un n y cierta diferencia con \\(H_{a}\\) de interés?"
  },
  {
    "objectID": "posts/01-concepto-y-modelos-experimentar/index.html#quiz-prueba-estadística-de-hipótesis",
    "href": "posts/01-concepto-y-modelos-experimentar/index.html#quiz-prueba-estadística-de-hipótesis",
    "title": "Conceptos y Modelos en la experimentación",
    "section": "Quiz: Prueba estadística de hipótesis",
    "text": "Quiz: Prueba estadística de hipótesis\nParticipa: vevox.app ID: 125-688-362"
  },
  {
    "objectID": "posts/09-medidas-repetidas/index.html",
    "href": "posts/09-medidas-repetidas/index.html",
    "title": "Modelos de medidas repetidas",
    "section": "",
    "text": "El tiempo es una variable problemática en el análisis estadístico, sobre todo por la necesidad de postular el supuesto de independencia entre las observaciones, lo que solemos asegurar aleatorizando las unidades experimentales. Las observaciones arregladas a lo largo del tiempo comúnmente no pueden aleatorizarse, por ejemplo cuando estamos dando seguimiento al crecimiento de un organismo. Por otro lado, también puede ocurrir esta falta de independencia por cercanía geográfica, así que el espacio comparte desafíos estadísticos con el tiempo."
  },
  {
    "objectID": "posts/09-medidas-repetidas/index.html#ejemplo-con-árboles-de-sitka",
    "href": "posts/09-medidas-repetidas/index.html#ejemplo-con-árboles-de-sitka",
    "title": "Modelos de medidas repetidas",
    "section": "Ejemplo con árboles de Sitka",
    "text": "Ejemplo con árboles de Sitka\nFuente: Venables y Ripley (1999, p.206), tabla Sitka de la biblioteca MASS. Datos de Diggle, Liang y Zeger (1994).\nSe trata de mediciones del tamaño-log (que se define como el logaritmo de la altura más dos veces el logaritmo del diámetro), de 79 árboles de Sitka spruce.\nA 54 de ellos se les hizo crecer en cámaras con atmósfera enriquecida con ozono y otros 25 fueron controles. La talla fue medida cinco veces en 1988 a intervalos de aproximadamente un mes (el tiempo se da en días a partir del 1 de enero de 1998). En 1989 se tomaron otras ocho mediciones (que se incluyen en una tabla aparte: Sitka89).\n\n\nCódigo\nlibrary(MASS)\nlibrary(nlme)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::collapse() masks nlme::collapse()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ dplyr::select()   masks MASS::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCódigo\nsitka88 &lt;- Sitka\n\n\n\n\nCódigo\nstr(sitka88)\n\n\n'data.frame':   395 obs. of  4 variables:\n $ size : num  4.51 4.98 5.41 5.9 6.15 4.24 4.2 4.68 4.92 4.96 ...\n $ Time : num  152 174 201 227 258 152 174 201 227 258 ...\n $ tree : int  1 1 1 1 1 2 2 2 2 2 ...\n $ treat: Factor w/ 2 levels \"control\",\"ozone\": 2 2 2 2 2 2 2 2 2 2 ...\n\n\nLa estructura de grupos podemos usarla para representar una curva de crecimiento por árbol. Los 79 árboles en los datos “sitka” son demasiados para el ejemplo que quiero ilustrar. Mostraré sólo dos árboles: el 64 y 24.\n\nExploración de los datos de Sitka\n\n\nCódigo\nsitka88 &lt;-groupedData(size ~ Time | tree, data=sitka88)\nplot(sitka88[sitka88$tree == 64 | sitka88$tree == 24, ])\n\n\n\n\n\n\n\n\n\n¿Cómo se ven los número de resumen de los datos en general?\n\n\nCódigo\nformula(sitka88)\n\n\nsize ~ Time | tree\n\n\n\n\nCódigo\nhead(gsummary(sitka88[, c(\"size\",\"Time\")], groups=sitka88$size, omit=TRUE))\n\n\n     size Time\n2.23 2.23  152\n2.79 2.79  152\n2.84 2.84  152\n2.89 2.89  174\n2.96 2.96  152\n2.99 2.99  152\n\n\n\n\nCódigo\nplot.design(size ~ treat, data=sitka88)\n\n\n\n\n\n\n\n\n\nPongo una línea de tendencia en la gráfica con la opción geom_smooth. Tengo multiples opciones, ve la ayuda, pero aquí consideré dos opciones loess con el parámetro span = 1. La otra opción que consideré fue el método gam. Esta solución es demandante en cuanto a número de datos necesarios. En este caso tuve que ajustar el parámetro de número de nudos (knots), pues la aproximación a una curva suave por el método aditivo generalizado usado, gam, requiere por defecto datos para por lo menos calcular 10 nudos y esto no se logra en este conjunto de datos. Usualmente funciona sin mayores problemas cuando se tienen más de 1000 puntos. Para tener una representación un poco más simple opté por eliminar los intervalos de confianza, eso lo controla el parámetro se.\n\n\nCódigo\noptions(repr.plot.width=12, repr.plot.height=6)\nlibrary(ggplot2)\nggplot(sitka88, aes(x=Time, y=size, color = tree)) + \n       geom_smooth(method = \"loess\", span = 1, formula = y ~ x, se = FALSE, show.legend = FALSE) +\n       geom_point(show.legend = FALSE) + facet_grid(. ~ treat) +\n       theme(strip.background = element_rect(fill=\"#ffe5cc\"),\n             text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\n\nCódigo\nggplot(sitka88, aes(x=Time, y=size, color = tree)) + \n       geom_smooth(method = \"gam\", span = 1, formula = y ~ s(x, bs = \"cs\", k = 5), \n                   se = FALSE, show.legend = FALSE) +\n       geom_point(show.legend = FALSE) + facet_grid(. ~ treat) +\n       theme(strip.background = element_rect(fill=\"#ffe5cc\"),\n             text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\nAhora ajusto el modelo completo con el tiempo. Fuerzo a que el tiempo sea tratado como un factor ordenado, lo que junto con la opción de contraste usada ajusta polinomios ortogonales en este caso.\n\n\nCódigo\noptions(contrasts=c(\"contr.treatment\", \"contr.poly\"))\n\n\n\n\nCódigo\nsitka.lme1 &lt;- lme(fixed = size ~ treat * ordered(Time),\n                  random = ~ 1 | tree,\n                  data = sitka88)\nsummary(sitka.lme1)$tTable\n\n\n                                 Value  Std.Error  DF    t-value       p-value\n(Intercept)                 4.98512000 0.12286970 308 40.5724123 1.332276e-125\ntreatozone                 -0.21115704 0.14861459  77 -1.4208365  1.594013e-01\nordered(Time).L             1.19711183 0.03221011 308 37.1657221 7.430810e-116\nordered(Time).Q            -0.13405824 0.03221011 308 -4.1619932  4.098121e-05\nordered(Time).C            -0.04085663 0.03221011 308 -1.2684413  2.055983e-01\nordered(Time)^4            -0.02729902 0.03221011 308 -0.8475297  3.973580e-01\ntreatozone:ordered(Time).L -0.17856562 0.03895909 308 -4.5834135  6.656627e-06\ntreatozone:ordered(Time).Q -0.02644698 0.03895909 308 -0.6788399  4.977491e-01\ntreatozone:ordered(Time).C -0.01424899 0.03895909 308 -0.3657423  7.148084e-01\ntreatozone:ordered(Time)^4  0.01240293 0.03895909 308  0.3183578  7.504293e-01\n\n\n\n\nCódigo\nintervals(sitka.lme1, level = 0.95)$fixed\n\n\n                                 lower        est.       upper\n(Intercept)                 4.74334979  4.98512000  5.22689021\ntreatozone                 -0.50708650 -0.21115704  0.08477242\nordered(Time).L             1.13373214  1.19711183  1.26049153\nordered(Time).Q            -0.19743793 -0.13405824 -0.07067854\nordered(Time).C            -0.10423632 -0.04085663  0.02252307\nordered(Time)^4            -0.09067872 -0.02729902  0.03608067\ntreatozone:ordered(Time).L -0.25522527 -0.17856562 -0.10190597\ntreatozone:ordered(Time).Q -0.10310663 -0.02644698  0.05021266\ntreatozone:ordered(Time).C -0.09090864 -0.01424899  0.06241066\ntreatozone:ordered(Time)^4 -0.06425672  0.01240293  0.08906258\nattr(,\"label\")\n[1] \"Fixed effects:\"\n\n\n\n\nAnaliizando la interección\nNotando la significancia de los términos de interacción: ¿podría simplificar el modelo limitando el ajuste a un efecto lineal de crecimiento que distingue entre los tratamiento? Veamos, calculo un nuevo vector que me permite hacer el ajuste de un efecto lineal a la diferencia entre tratamientos (que es la interacción).\n\n\nCódigo\nsitka88$tratGrad &lt;- sitka88$Time * (sitka88$treat==\"ozone\")\n\n\n\n\nCódigo\ntapply(sitka88$Time,list(sitka88$treat, sitka88$Time), FUN=mean)\n\n\n        152 174 201 227 258\ncontrol 152 174 201 227 258\nozone   152 174 201 227 258\n\n\nCódigo\ntapply(sitka88$tratGrad,list(sitka88$treat, sitka88$Time), FUN=mean)\n\n\n        152 174 201 227 258\ncontrol   0   0   0   0   0\nozone   152 174 201 227 258\n\n\nAhora ajusto un modelo en el que elimino la interacción de tratamiento con tiempo. Al mismo tiempo substituyo este efecto por el modelo con tiempo lineal en interacción con el tratamiento ozono (la variable que acabo de construir).\n\n\nCódigo\nsitka.lme2 &lt;- update(sitka.lme1, \n                     fixed = size ~ ordered(Time) + treat + tratGrad)\nsummary(sitka.lme2)$tTable\n\n\n                      Value    Std.Error  DF    t-value       p-value\n(Intercept)      4.98512000 0.1228696967 311 40.5724123 2.952779e-126\nordered(Time).L  1.19755089 0.0320437966 311 37.3723158 4.821617e-117\nordered(Time).Q -0.14549447 0.0181029428 311 -8.0370620  1.933996e-14\nordered(Time).C -0.05059644 0.0180459272 311 -2.8037597  5.368534e-03\nordered(Time)^4 -0.01672449 0.0180516171 311 -0.9264818  3.549141e-01\ntreatozone       0.22167749 0.1756140543  77  1.2622992  2.106510e-01\ntratGrad        -0.00213851 0.0004622668 311 -4.6261386  5.473912e-06\n\n\nEl resumen del ajuste muestra dos criterios que no hemos comentado mayormente antes. Son útiles para comparar y evaluar modelos. Estas medidas son resultado de la búsqueda de alternativas para valorar modelos que no se centre en el famoso valor de p.\n\nAIC - Criterio de información de Akaike = -2 * logVerosimilitud + 2 numParámetros\nBIC - Criterio de información bayesiano = -2 * logVerosimilitud + numParámetros * log(N)\n\nEs bueno contar con ellos para comparar la calidad general de los modelos ajustados, pero no olvides que centrar nuestra atención en los intervalos de confianza es más informativo y potencialmente interesante.\nEn cualquier caso, “entre más pequeño el valor del criterio, mejor”.\n\n\nCódigo\ndata.frame(AICmodelo_red=summary(sitka.lme2)$AIC, AICmodelo_comp=summary(sitka.lme1)$AIC)\n\n\n  AICmodelo_red AICmodelo_comp\n1       69.2691       79.90098\n\n\nCódigo\ndata.frame(BICmodelo_red=summary(sitka.lme2)$BIC, BICmodelo_comp=summary(sitka.lme1)$BIC)\n\n\n  BICmodelo_red BICmodelo_comp\n1      104.9181       127.3399\n\n\nTanto el criterio AIC como el BIC sugieren que el modelo reducido es preferible al modelo completo inicial. ¿Qué sugiere la comparación, en devianzas, de ambos modelo?\n\n\nCódigo\nanova(sitka.lme1, sitka.lme2)\n\n\nWarning in anova.lme(sitka.lme1, sitka.lme2): fitted objects with different\nfixed effects. REML comparisons are not meaningful.\n\n\n           Model df      AIC      BIC    logLik   Test  L.Ratio p-value\nsitka.lme1     1 12 79.90098 127.3399 -27.95049                        \nsitka.lme2     2  9 69.26910 104.9181 -25.63455 1 vs 2 4.631882  0.2008\n\n\nNotese la advertencia que aparece al intentar esta comparación. Para resolverla, hay que volver a ajustar los modelos de interés, pero ahora con el método “ML”, que si me permite hacer comparaciones entre modelos.\n\n\nCódigo\nsitka.lme1.ML &lt;- lme(fixed = size ~ treat * ordered(Time),\n                 random = ~ 1 | tree,\n                 data = sitka88, method=\"ML\")\n\nsitka.lme2.ML &lt;- update(sitka.lme1.ML, \n                     fixed = size ~ ordered(Time) + treat + tratGrad, method=\"ML\")\n\n\nComparemos los resultados obtenidos hasta aquí\n\n\nCódigo\nanova(sitka.lme1.ML, sitka.lme2.ML)\n\n\n              Model df      AIC      BIC    logLik   Test   L.Ratio p-value\nsitka.lme1.ML     1 12 30.94633 78.69296 -3.473163                         \nsitka.lme2.ML     2  9 25.43446 61.24443 -3.717228 1 vs 2 0.4881304  0.9215\n\n\n\n\nCódigo\nsummary(sitka.lme2.ML)\n\n\nLinear mixed-effects model fit by maximum likelihood\n  Data: sitka88 \n       AIC      BIC    logLik\n  25.43446 61.24443 -3.717228\n\nRandom effects:\n Formula: ~1 | tree\n        (Intercept)  Residual\nStdDev:    0.602333 0.1591217\n\nFixed effects:  size ~ ordered(Time) + treat + tratGrad \n                    Value  Std.Error  DF  t-value p-value\n(Intercept)      4.985120 0.12239376 311 40.73018  0.0000\nordered(Time).L  1.197551 0.03207475 311 37.33625  0.0000\nordered(Time).Q -0.145494 0.01812043 311 -8.02931  0.0000\nordered(Time).C -0.050596 0.01806336 311 -2.80105  0.0054\nordered(Time)^4 -0.016724 0.01806906 311 -0.92559  0.3554\ntreatozone       0.221677 0.17517548  77  1.26546  0.2095\ntratGrad        -0.002139 0.00046271 311 -4.62167  0.0000\n Correlation: \n                (Intr) o(T).L o(T).Q o(T).C o(T)^4 tretzn\nordered(Time).L  0.000                                   \nordered(Time).Q  0.000  0.066                            \nordered(Time).C  0.000  0.000  0.000                     \nordered(Time)^4  0.000  0.021  0.002  0.000              \ntreatozone      -0.699  0.442  0.042  0.000  0.013       \ntratGrad         0.000 -0.826 -0.079  0.000 -0.025 -0.535\n\nStandardized Within-Group Residuals:\n         Min           Q1          Med           Q3          Max \n-2.631308932 -0.525058743  0.009619032  0.514425152  5.968135202 \n\nNumber of Observations: 395\nNumber of Groups: 79"
  },
  {
    "objectID": "posts/09-medidas-repetidas/index.html#selección-del-polinomio",
    "href": "posts/09-medidas-repetidas/index.html#selección-del-polinomio",
    "title": "Modelos de medidas repetidas",
    "section": "Selección del polinomio",
    "text": "Selección del polinomio\nAhora veamos un poco más de cerca el modelo y veamos si la respuesta muestra una curvatura que pueda ser aproximada entonces por un polinomio y en ese caso identificar el polinomio de menor grado que podríamos usar.\nRecuerden que al utilizar factores ordenados le estamos indicando a R que optaremos por contrastes polinomiales ortogonales. Otra manera de obtener estos contrastes es con la función poly. Para ver como funciona esto usemos este comando los datos de tiempo. Del vector Time poly produce cuatro columnas nuevas, que dan cuenta de la tendencia lineal, cuadrática, cúbica, etc., con la peculiaridad de que cada columna es ortogonal a las demás.\n\n\nCódigo\nhead(poly(sitka88$Time, 4))\n\n\n                1           2           3            4\n[1,] -0.067556996  0.05935630 -0.04131435  0.018236996\n[2,] -0.038067831 -0.01959476  0.06928561 -0.059101377\n[3,] -0.001876583 -0.05989079  0.01348825  0.079713367\n[4,]  0.032974248 -0.03974302 -0.07127418 -0.048782834\n[5,]  0.074527162  0.05987226  0.02981468  0.009933848\n[6,] -0.067556996  0.05935630 -0.04131435  0.018236996\n\n\nEl Modelo 2 que hemos ajustado consumió todos los grados de libertad posible y estimó un polinomio de grado 4. Consideremos sólo el polinomio cúbico.\n\n\nCódigo\nsitka.lme3.ML &lt;- lme(fixed = size ~ treat + poly(Time, 3) + tratGrad,\n                 random = ~ 1 | tree,\n                 data = sitka88, method =\"ML\")\n\n\n\n\nCódigo\nsummary(sitka.lme3.ML)$tTable\n\n\n                     Value    Std.Error  DF    t-value       p-value\n(Intercept)     4.98512000 0.1222363435 312  40.782634 4.608733e-127\ntreatozone      0.22167749 0.1752587841  77   1.264858  2.097370e-01\npoly(Time, 3)1 10.55436327 0.2867893021 312  36.801803 1.551949e-115\npoly(Time, 3)2 -1.90581863 0.1613315870 312 -11.813053  7.256641e-27\npoly(Time, 3)3 -0.25924189 0.1613315870 312  -1.606889  1.090903e-01\ntratGrad       -0.00213851 0.0004649641 312  -4.599303  6.169978e-06\n\n\nAhora podemos compararlo con el modelo “2” que ajustamos antes, para explorar si el nuevo modelo pierde una grado importante de capacidad explicativa.\n\n\nCódigo\nanova(sitka.lme2.ML, sitka.lme3.ML)\n\n\n              Model df      AIC      BIC    logLik   Test L.Ratio p-value\nsitka.lme2.ML     1  9 25.43446 61.24443 -3.717228                       \nsitka.lme3.ML     2  8 27.31449 59.14557 -5.657243 1 vs 2 3.88003  0.0489\n\n\nEl crecimiento promedio de los árboles a lo largo del tiempo se puede ver así, aunque esto no considera la variación debida a los árboles en lo individual. No obstante veamos el resultado general.\n\n\nCódigo\ntapply(fitted(sitka.lme3.ML), list(sitka88$treat, sitka88$Time), mean)\n\n\n             152      174      201      227      258\ncontrol 4.169687 4.602721 5.075958 5.427362 5.649872\nozone   4.066311 4.452297 4.867795 5.163598 5.319814\n\n\nMás adecuado es utilizar la función predict() para considerar las particularidades del modelo para hacer las predicciones. Estos resultados los pondremos en una gráfica para ver de mejor manera los resultados.\n\n\nCódigo\nsitka88$ajus &lt;- predict(sitka.lme3.ML)\n\n\n\n\nCódigo\nggplot(sitka88, aes(x=Time, y=ajus, color = tree)) + \n       geom_point(show.legend = FALSE) + facet_grid(. ~ treat) +\n       geom_line(show.legend = FALSE) + \n       theme(strip.background = element_rect(fill=\"#ffe5cc\"),\n             text = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nComponente aleatorio\nPodemos ahora explorar como mejorar la modelación de los componentes aleatorios en este modelo.\n\n\nCódigo\nVarCorr(sitka.lme1)\n\n\ntree = pdLogChol(1) \n            Variance   StdDev   \n(Intercept) 0.37223660 0.6101120\nResidual    0.02593727 0.1610505\n\n\nLos modelos pueden incorporar una estructura de modelación de los patrones de correlación entre las observaciones. En este caso derivadas del hecho de que las mediciones se realizan a lo largo del tiempo, en intervalos relativamente cortos, sobre el mismo sujeto. Haremos esto aquí solo para ejemplificar el tema, que es amplio. Mi recomendación es más bien recurrir a la literatura existente para profundizar en el tema. Nótese que para la comparación de modelos en donde no estamos cambiando los componentes fijo, puede hacerse aún cuando el método de ajuste sea el REML. Usamos una opción del patrón de correlación que estamos asumiendo mediante la opción cor que recibe una estructura que da cuenta del patrón de correlación que se asume afecta a la forma como se producen las observaciones. En este caso optamos por un proceso de autocorrelación de orden 1 en las observaciones con correlación de 70%, derivado de medir a lo largo del tiempo cada árbol. Esto es lo que hace la función corCAR1, sobre la que pueden encontrar más información en la ayuda de R.\n\n\nCódigo\nsitka.lme4 &lt;- lme(size~ treat * ordered(Time), random = ~ 1 | tree,\n                  data = sitka88, corr=corCAR1(0.7, ~ Time | tree))\n\n\nVeamos como cambian los estadísticos de los modelos. Comparemos el modelo completo inicial, contra el completo considerando la nueva información sobre la correlación que hemos agregado en el modelo 4.\n\n\nCódigo\nanova(sitka.lme1,sitka.lme4)\n\n\n           Model df       AIC      BIC    logLik   Test  L.Ratio p-value\nsitka.lme1     1 12  79.90098 127.3399 -27.95049                        \nsitka.lme4     2 13 -63.17167 -11.7795  44.58583 1 vs 2 145.0727  &lt;.0001\n\n\nSe ve raro que haya AIC y BIC negativos, pero pasa, sí los consideraríamos como valores más pequeños que los positivos, así que aquí, el modelo 4 parece tener un ajuste bastante mejor que el 1.\n¿cómo se ve el modelo ajustado finalmente?\n\n\nCódigo\nsummary(sitka.lme4)$tTable\n\n\n                                 Value  Std.Error  DF    t-value       p-value\n(Intercept)                 4.98512000 0.12635223 308 39.4541523 1.827148e-122\ntreatozone                 -0.21115704 0.15282682  77 -1.3816753  1.710673e-01\nordered(Time).L             1.19711183 0.04907128 308 24.3953674  6.282539e-74\nordered(Time).Q            -0.13405824 0.02642497 308 -5.0731653  6.774439e-07\nordered(Time).C            -0.04085663 0.01978964 308 -2.0645467  3.980280e-02\nordered(Time)^4            -0.02729902 0.01672573 308 -1.6321568  1.036684e-01\ntreatozone:ordered(Time).L -0.17856562 0.05935318 308 -3.0085264  2.841806e-03\ntreatozone:ordered(Time).Q -0.02644698 0.03196179 308 -0.8274562  4.086192e-01\ntreatozone:ordered(Time).C -0.01424899 0.02393616 308 -0.5952914  5.520859e-01\ntreatozone:ordered(Time)^4  0.01240293 0.02023028 308  0.6130875  5.402709e-01\n\n\nCódigo\nsummary(sitka.lme1)$tTable\n\n\n                                 Value  Std.Error  DF    t-value       p-value\n(Intercept)                 4.98512000 0.12286970 308 40.5724123 1.332276e-125\ntreatozone                 -0.21115704 0.14861459  77 -1.4208365  1.594013e-01\nordered(Time).L             1.19711183 0.03221011 308 37.1657221 7.430810e-116\nordered(Time).Q            -0.13405824 0.03221011 308 -4.1619932  4.098121e-05\nordered(Time).C            -0.04085663 0.03221011 308 -1.2684413  2.055983e-01\nordered(Time)^4            -0.02729902 0.03221011 308 -0.8475297  3.973580e-01\ntreatozone:ordered(Time).L -0.17856562 0.03895909 308 -4.5834135  6.656627e-06\ntreatozone:ordered(Time).Q -0.02644698 0.03895909 308 -0.6788399  4.977491e-01\ntreatozone:ordered(Time).C -0.01424899 0.03895909 308 -0.3657423  7.148084e-01\ntreatozone:ordered(Time)^4  0.01240293 0.03895909 308  0.3183578  7.504293e-01\n\n\nAunque hay obviamente una importante correlaciónn entre observaciones, el efecto de considerar esto en el modelo es mínimo en términos de los valores de los coeficientes, aunque la significación valorada en términos de p cambia un poco, pero nada que nos haga modificar la apreciación del modelo. No parece valer la pena incorporar este aspecto de autocorrelación en el ajuste final, si nos atenemos a preferir el modelo más simple. Por otro lado, el asunto de considerar un efecto de autocorrelación en las observaciones parece exigir ser considerado. Tomemos este último camino\nLos intervalos de confianza de los coeficientes del modelo 4 son estos:\n\n\nCódigo\nintervals(sitka.lme4, which = \"fixed\", level = 0.95)$fixed\n\n\n                                 lower        est.        upper\n(Intercept)                 4.73649723  4.98512000  5.233742772\ntreatozone                 -0.51547411 -0.21115704  0.093160032\nordered(Time).L             1.10055448  1.19711183  1.293669187\nordered(Time).Q            -0.18605455 -0.13405824 -0.082061932\nordered(Time).C            -0.07979661 -0.04085663 -0.001916640\nordered(Time)^4            -0.06021018 -0.02729902  0.005612139\ntreatozone:ordered(Time).L -0.29535464 -0.17856562 -0.061776597\ntreatozone:ordered(Time).Q -0.08933808 -0.02644698  0.036444111\ntreatozone:ordered(Time).C -0.06134807 -0.01424899  0.032850096\ntreatozone:ordered(Time)^4 -0.02740411  0.01240293  0.052209970\nattr(,\"label\")\n[1] \"Fixed effects:\"\n\n\n\n\nCódigo\nsitka.fin &lt;- aggregate(list(ajustado=fitted(sitka.lme4)), \n                       list(tiempo=sitka88$Time, trat=sitka88$treat), FUN=mean)\n\n\n\n\nCódigo\nsitka.fin$tiempo &lt;- as.numeric(sitka.fin$tiempo)\n\n\nUna gráfica de los resultados podría ser así. Ilustra la regresión obtenida para cada tratamiento y añado los puntos observados ( para que se vean un poco mejor use el geoma “jitter” que grafica los puntos pero procurando que no se sobrepongan. Le pedí que lo hicieran en “bandas” de ancho 2.\n\n\nCódigo\nggplot(sitka.fin, aes(x=tiempo, y=ajustado, color = trat)) + \n       geom_line(show.legend = TRUE) + \n       geom_point(show.legend = FALSE) + \n       xlab(label = \"tamaño-log\") +\n       ylab(label = \"tiempo (días)\") + \n       theme(strip.background = element_rect(fill=\"#ffe5cc\"),\n             text = element_text(size = 20)) +\n\n       # componente que agrega los datos a la gráfica\n       geom_jitter(data = sitka88, width = 2, \n                   mapping = aes(x = Time, y = size, color = treat)) \n\n\n\n\n\n\n\n\n\nConstruir los intervalos de confianza a partir del modelo de efectos mixtos puede ser un poco más elaborado, así que a continuación muestro como pueden hacerse. Una posibilidad es usar la función intervalscon la opción which = “fixed” para recuperar los resultados que implica sólo a los componentes de efectos fijos del modelo, que son los que se involucran en la predicción (los aleatorios participan en las varianzas).\n\n\nCódigo\nlibrary(tidyverse, warn.conflicts = FALSE)\nsitka_intconf &lt;- tibble(Time = sitka88$Time, treat = sitka88$treat)\nsitka_intconf &lt;- sitka_intconf %&gt;% add_column(ajus = fitted(sitka.lme4, level = 0))\nhead(sitka_intconf)\n\n\n# A tibble: 6 × 3\n   Time treat  ajus\n  &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n1   152 ozone  4.06\n2   174 ozone  4.47\n3   201 ozone  4.85\n4   227 ozone  5.18\n5   258 ozone  5.31\n6   152 ozone  4.06\n\n\nNecesitaremos la matriz de diseño para calcular los intervalos de confianza asociados con el mmodelo.\n\n\nCódigo\nDesignmat &lt;- model.matrix(eval(eval(sitka.lme4$call$fixed)[-2]), \n                          sitka_intconf[-ncol(sitka_intconf)])\n\n\nAhora calculamos los errores estándar de las predicciones. La matriz diseño contiene las variables indicadoras de todos los términos en el modelo. Al multiplicarla por la matriz de varianzas y covarianzas del modelo (la que está en el componente sitka.lme4$varFix del modelo ajustado), produce los estimadores de varianza requeridos\n\n\nCódigo\npredvar &lt;- diag(Designmat %*% sitka.lme4$varFix %*% t(Designmat))\nsitka_intconf$SE &lt;- sqrt(predvar) \n\n\n\n\nCódigo\nhead(sitka_intconf)\n\n\n# A tibble: 6 × 4\n   Time treat  ajus     SE\n  &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1   152 ozone  4.06 0.0880\n2   174 ozone  4.47 0.0880\n3   201 ozone  4.85 0.0880\n4   227 ozone  5.18 0.0880\n5   258 ozone  5.31 0.0880\n6   152 ozone  4.06 0.0880\n\n\nSolo resta agregar las bandas de confianza en torno a la egresión. Esto lo haré con el geoma “ribbon” de ggplot2. Esta será una gráfica compleja que se elabora a partir de tres tablas de datos.\n\n\nCódigo\nggplot(sitka.fin, aes(x=tiempo, y=ajustado, color = trat)) + \n       geom_line(show.legend = TRUE) + \n\n       # Etiquetas y formato de despliegue\n       xlab(label = \"tamaño-log\") +\n       ylab(label = \"tiempo (días)\") + \n       theme(strip.background = element_rect(fill=\"#ffe5cc\"),\n             text = element_text(size = 20)) +\n\n       # bandas de confianza\n       geom_ribbon(data = sitka_intconf, aes(x = Time, y = ajus, color = treat,\n                                             ymin = ajus - 2 * SE,\n                                             ymax = ajus + 2 * SE),\n                   alpha=0.2, fill = \"blue\") +       \n\n       # componente que agrega los datos a la gráfica\n       geom_jitter(data = sitka88, width = 2, \n                   mapping = aes(x = Time, y = size, color = treat))"
  },
  {
    "objectID": "posts/10-experimentar/index.html",
    "href": "posts/10-experimentar/index.html",
    "title": "Experimentación Ecológica",
    "section": "",
    "text": "La experimentación con diseños adecuadamente aleatorizados es la norma de referencia científica para la exploración de proposiciones causales. Preguntas de gran envergadura para los intereses humanos se resuelven mediante esta aproximación. La experimentación desafía la imaginación y las capacidades materiales y humanas. Exige ingenio y creatividad, pero también está acotada por el marco ético que nos exige evitar dañar a otros seres humanos, así como también evitar hacerlo a otras especies o a la naturaleza misma."
  },
  {
    "objectID": "posts/10-experimentar/index.html#ciencia-abierta",
    "href": "posts/10-experimentar/index.html#ciencia-abierta",
    "title": "Experimentación Ecológica",
    "section": "Ciencia Abierta",
    "text": "Ciencia Abierta\nCentro de Ciencia Abierta\nCiecia abierta - UNESCO\nCiencia Abierta - CEPAL"
  }
]