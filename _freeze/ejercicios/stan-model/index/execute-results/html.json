{
  "hash": "a1a61b5eaeb505eeaf70010c6bee3310",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Modelación con Stan\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n:::\n\n\n\n\nLa modelación estadística tiene múltiples opciones actualmente. Señaladamente **R**, **Python**, **Julia**, **Matlab**, *Stata* y **Stan** que de acuerdo con su propia apreciación \"es una plataforma de última generación para el modelado estadístico y el cálculo estadístico de alto rendimiento en la que confían miles de usuarios\". **Stan** es básicamente un lenguaje para la programación de modelos probabilísticos que permite:\n\n+ Inferencia estadística bayesiana completa.\n+ Inferencia bayesiana aproximada con inferencia variacional.\n+ Estimación de máxima verosimilitud penalizada con optimización.\n\n**Stan** tiene su motor de cálculo propio, pero  interactúa muy bien con los lenguajes de análisis de datos más populares (*R*, *Python*, *MATLAB*, etc.) y en los sistemas operativos comunes (_Linux_, _Mac_, _Windows_). Puedes obtener más información sobre esta propuesta de modelación estadística en la [página de Stan](https://mc-stan.org/).\n\n\n## Preparación\n\nEl concepto que utiliza **Stan** parte de la especificación de las funciones de densidad que le interesan al usuario para enseguida ajustar los modelos a los datos. Un ejemplo trivial es el siguiente, en el que sólo nos proponemos estimar la media de una muestra de datos que asumiremos se distribuyen normalmente. Lo primero que hay que hacer es crear un archivo que especifique el modelo en los términos que **Stan** requiere. La especificación básica es la siguiente (se pueden agregar comentarios con un doble diagonal). Primero una definición del tipo de datos que requiere el modelo.\n\n``` stan\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int<lower=0> N;\n  vector[N] y;\n}\n```\n\nEn seguida se detallan los parámetros que definen el modelo.\n\n``` stan\n// The parameters accepted by the model. Our model\n// accepts two parameters 'mu' and 'sigma'.\nparameters {\n  real mu;\n  real<lower=0> sigma;\n}\n```\n\nFinalmente, se especifica el modelo. El modelo completo se guarda como un archivo *stan* al que se llamará cuando se busque realizar el ajuste.\n\n\n``` stan\n// The model to be estimated. We model the output\n// 'y' to be normally distributed with mean 'mu'\n// and standard deviation 'sigma'.\nmodel {\n  y ~ normal(mu, sigma);\n}\n\n```\n\nPara ejemplificar el uso del modelo `stan` ya definido arriba, sólo nos resta preparar algunos datos de prueba, activar la biblioteca `rstan` y realizar el ajuste, que básicamente consiste en generar un muestreo de la distribución de probabilidades. Al invocar la función `stan` se _compila_ la especificación del modelo para que **Stan** lo pueda procesar. Una vez hecho eso, se realiza un proceso de muestreo de las distribución conjunta especificada, lo qe equivale a *ajustar el modelo* en **Stan**.\n\n\n\n\n::: {.cell}\n\n```{.r .bg-warning .cell-code}\nlibrary(rstan)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nCargando paquete requerido: StanHeaders\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nrstan version 2.32.6 (Stan version 2.32.2)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDo not specify '-march=native' in 'LOCAL_CPPFLAGS' or a Makevars file\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAdjuntando el paquete: 'rstan'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:tidyr':\n\n    extract\n```\n\n\n:::\n\n```{.r .bg-warning .cell-code}\nlibrary(tibble)\nrstan_options(auto_write = TRUE)\n\ndatos <-  list(N = 1000, \n               y = rnorm(1000, 10, 2))\n               \nnorm_fit <- stan(file = 'modelo.stan', data = datos)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 4.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.45 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.046 seconds (Warm-up)\nChain 1:                0.045 seconds (Sampling)\nChain 1:                0.091 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.043 seconds (Warm-up)\nChain 2:                0.054 seconds (Sampling)\nChain 2:                0.097 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 5.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.52 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.051 seconds (Warm-up)\nChain 3:                0.042 seconds (Sampling)\nChain 3:                0.093 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.3e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.069 seconds (Warm-up)\nChain 4:                0.057 seconds (Sampling)\nChain 4:                0.126 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\nUn objeto `stanfit` en **R** contiene los resultados derivados de ajustar (muestrear) un modelo **Stan** utilizando métodos Monte Carlo para cadenas de Markov (es la que se usa por default) o alguna de las _aproximaciones variacionales_ que es capaz de procesar Stan.\n\nAhora podemos ver los resultados del ajuste del modelo y explorar los resultados que arroja.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(norm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$summary\n              mean      se_mean         sd         2.5%          25%\nmu        9.985783 0.0011355597 0.06614853     9.855086     9.941181\nsigma     2.026648 0.0007744342 0.04421861     1.945260     1.996270\nlp__  -1205.628069 0.0248991890 1.03721594 -1208.246434 -1206.024870\n               50%          75%       97.5%    n_eff      Rhat\nmu        9.986748    10.030374    10.11353 3393.285 0.9998499\nsigma     2.025854     2.055898     2.11594 3260.177 1.0000536\nlp__  -1205.314867 -1204.911103 -1204.64576 1735.274 1.0005728\n\n$c_summary\n, , chains = chain:1\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu        9.985182 0.06568395     9.856185     9.940864     9.984545\n    sigma     2.025361 0.04386068     1.947783     1.993357     2.024739\n    lp__  -1205.614593 0.96288322 -1208.149782 -1206.029611 -1205.315540\n         stats\nparameter          75%        97.5%\n    mu       10.032699    10.110996\n    sigma     2.053814     2.110686\n    lp__  -1204.914020 -1204.641254\n\n, , chains = chain:2\n\n         stats\nparameter        mean         sd         2.5%          25%          50%\n    mu        9.98513 0.06777224     9.845464     9.941350     9.986006\n    sigma     2.02700 0.04344609     1.946390     1.996806     2.025546\n    lp__  -1205.63887 1.17127274 -1208.395582 -1205.974116 -1205.285256\n         stats\nparameter          75%        97.5%\n    mu       10.028296    10.118765\n    sigma     2.055042     2.114998\n    lp__  -1204.892783 -1204.646001\n\n, , chains = chain:3\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu        9.986934 0.06591295     9.862552     9.941039     9.987669\n    sigma     2.028308 0.04538603     1.945901     1.998069     2.028165\n    lp__  -1205.646790 0.99467206 -1208.199104 -1206.054733 -1205.359767\n         stats\nparameter          75%        97.5%\n    mu       10.032883    10.110454\n    sigma     2.057687     2.122019\n    lp__  -1204.938112 -1204.657286\n\n, , chains = chain:4\n\n         stats\nparameter         mean         sd         2.5%          25%          50%\n    mu        9.985884 0.06528054     9.856620     9.941761     9.987575\n    sigma     2.025920 0.04416708     1.945046     1.996571     2.025995\n    lp__  -1205.612018 1.00856061 -1208.428302 -1206.005838 -1205.309293\n         stats\nparameter          75%        97.5%\n    mu       10.027844    10.115235\n    sigma     2.056599     2.115486\n    lp__  -1204.895888 -1204.644620\n```\n\n\n:::\n\n```{.r .cell-code}\najuste <- as_tibble(rstan::extract(norm_fit))\n\nhead(ajuste)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n         mu     sigma      lp__\n  <dbl[1d]> <dbl[1d]> <dbl[1d]>\n1      9.93      2.02    -1205.\n2      9.98      2.07    -1205.\n3     10.1       1.96    -1209.\n4     10.1       2.06    -1206.\n5      9.96      2.07    -1205.\n6     10.0       1.93    -1207.\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(as_tibble(as.data.frame(norm_fit)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n     mu sigma   lp__\n  <dbl> <dbl>  <dbl>\n1  9.97  2.04 -1205.\n2  9.92  2.00 -1205.\n3  9.92  2.00 -1205.\n4 10.1   2.06 -1206.\n5 10.1   2.03 -1205.\n6  9.97  2.03 -1205.\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(ajuste$mu)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 9.985783\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}